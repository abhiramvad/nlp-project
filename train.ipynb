{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install torch transformers datasets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f935a432cec4dcc8b1f8f7a5d209cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a680a10b9402454285fd723e4209ffb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'text', 'date', 'metadata'],\n",
       "        num_rows: 6315233\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"open-web-math/open-web-math\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the dataset:\n",
      "{'url': 'https://telescoper.wordpress.com/2010/11/23/bayes-and-hi-theorem/', 'text': 'Bayes and his\\xa0Theorem\\n\\nMy earlier post on Bayesian probability seems to have generated quite a lot of readers, so this lunchtime I thought Iâ€™d add a little bit of background. The previous discussion started from the result\\n\\n$P(B|AC) = K^{-1}P(B|C)P(A|BC) = K^{-1} P(AB|C)$\\n\\nwhere\\n\\n$K=P(A|C).$\\n\\nAlthough this is called Bayesâ€™ theorem, the general form of it as stated here was actually first written down, not by Bayes but by Laplace. What Bayesâ€™ did was derive the special case of this formula for â€œinvertingâ€ the binomial distribution. This distribution gives the probability of x successes in n independent â€œtrialsâ€ each having the same probability of success, p; each â€œtrialâ€ has only two possible outcomes (â€œsuccessâ€ or â€œfailureâ€). Trials like this are usually called Bernoulli trials, after Daniel Bernoulli. If we ask the question â€œwhat is the probability of exactly x successes from the possible n?â€, the answer is given by the binomial distribution:\\n\\n$P_n(x|n,p)= C(n,x) p^x (1-p)^{n-x}$\\n\\nwhere\\n\\n$C(n,x)= n!/x!(n-x)!$\\n\\nis the number of distinct combinations of x objects that can be drawn from a pool of n.\\n\\nYou can probably see immediately how this arises. The probability of x consecutive successes is p multiplied by itself x times, or px. The probability of (n-x) successive failures is similarly (1-p)n-x. The last two terms basically therefore tell us the probability that we have exactly x successes (since there must be n-x failures). The combinatorial factor in front takes account of the fact that the ordering of successes and failures doesnâ€™t matter.\\n\\nThe binomial distribution applies, for example, to repeated tosses of a coin, in which case p is taken to be 0.5 for a fair coin. A biased coin might have a different value of p, but as long as the tosses are independent the formula still applies. The binomial distribution also applies to problems involving drawing balls from urns: it works exactly if the balls are replaced in the urn after each draw, but it also applies approximately without replacement, as long as the number of draws is much smaller than the number of balls in the urn. I leave it as an exercise to calculate the expectation value of the binomial distribution, but the result is not surprising: E(X)=np. If you toss a fair coin ten times the expectation value for the number of heads is 10 times 0.5, which is five. No surprise there. After another bit of maths, the variance of the distribution can also be found. It is np(1-p).\\n\\nSo this gives us the probability of x given a fixed value of p. Bayes was interested in the inverse of this result, the probability of p given x. In other words, Bayes was interested in the answer to the question â€œIf I perform n independent trials and get x successes, what is the probability distribution of p?â€. This is a classic example of inverse reasoning. He got the correct answer, eventually, but by very convoluted reasoning. In my opinion it is quite difficult to justify the name Bayesâ€™ theorem based on what he actually did, although Laplace did specifically acknowledge this contribution when he derived the general result later, which is no doubt why the theorem is always named in Bayesâ€™ honour.\\n\\nThis is not the only example in science where the wrong personâ€™s name is attached to a result or discovery. In fact, it is almost a law of Nature that any theorem that has a name has the wrong name. I propose that this observation should henceforth be known as Colesâ€™ Law.\\n\\nSo who was the mysterious mathematician behind this result? Thomas Bayes was born in 1702, son of Joshua Bayes, who was a Fellow of the Royal Society (FRS) and one of the very first nonconformist ministers to be ordained in England. Thomas was himself ordained and for a while worked with his father in the Presbyterian Meeting House in Leather Lane, near Holborn in London. In 1720 he was a minister in Tunbridge Wells, in Kent. He retired from the church in 1752 and died in 1761. Thomas Bayes didnâ€™t publish a single paper on mathematics in his own name during his lifetime but despite this was elected a Fellow of the Royal Society (FRS) in 1742. Presumably he had Friends of the Right Sort. He did however write a paper on fluxions in 1736, which was published anonymously. This was probably the grounds on which he was elected an FRS.\\n\\nThe paper containing the theorem that now bears his name was published posthumously in the Philosophical Transactions of the Royal Society of London in 1764.\\n\\nP.S. I understand that the authenticity of the picture is open to question. Whoever it actually is, he looks\\xa0 to me a bit like Laurence Olivierâ€¦\\n\\n11 Responses to â€œBayes and his\\xa0Theoremâ€\\n\\n1. Bryn Jones Says:\\n\\nThe Royal Society is providing free access to electronic versions of its journals until the end of this month. Readers of this blog might like to look at Thomas Bayesâ€™s two posthumous publications in the Philosophical Transactions.\\n\\nThe first is a short paper about series. The other is the paper about statistics communicated by Richard Price. (The statistics paper may be accessible on a long-term basis because it is one of the Royal Societyâ€™s Trailblazing papers the society provides access to as part of its 350th anniversary celebrations.)\\n\\nIncidentally, both Thomas Bayes and Richard Price were buried in the Bunhill Fields Cemetery in London and their tombs can be seen there today.\\n\\n2. Steve Warren Says:\\n\\nYou may be remembered in history as the discoverer of coleslaw, but you werenâ€™t the first.\\n\\nâ€¢ Anton Garrett Says:\\n\\nFor years I thought it was â€œcold slawâ€ because it was served cold. A good job I never asked for warm slaw.\\n\\n3. telescoper Says:\\n\\nMy surname, in Spanish, means â€œCabbagesâ€. So it was probably one of my ancestors who invented the chopped variety.\\n\\n4. Anton Garrett Says:\\n\\nThomas Bayes is now known to have gone to Edinburgh University, where his name appears in the records. He was barred from English universities because his nonconformist family did not have him baptised in the Church of England. (Charles Darwinâ€™s nonconformist family covered their bets by having baby Charles baptised in the CoE, although perhaps they believed it didnâ€™t count as a baptism since Charles had no say in it. Tist is why he was able to go to Christâ€™s College, Cambridge.)\\n\\n5. â€œColeâ€ is an old English word for cabbage, which survives in â€œcole slawâ€. The German word is â€œKohlâ€. (Somehow, I donâ€™t see PM or President Cabbage being a realistic possibility. ðŸ™‚ )\\n\\nNote that Old King Cole is unrelated (etymologically). Of course, this discussion could cause Peter to post a clip of\\n\\nNat â€œKingâ€ Cole\\n(guess what his real surname is).\\n\\nTo remind people to pay attention to spelling when they hear words, weâ€™ll close with the Quote of the Day:\\n\\nItâ€™s important to pay close attention in school. For years I thought that\\nbears masturbated all winter.\\n\\nâ€”Damon R. Milhem\\n\\n6. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\n7. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\nThe first typo was my fault; the extra linebreaks in the second attempt\\n(tested again here) appear to be a new â€œfeatureâ€.\\n\\n8. telescoper Says:\\n\\nThe noun â€œcoleâ€ can be found in English dictionaries as a generic name for plants of the cabbage family. It is related to the German kohl and scottish kail or kale. These are all derived from the latin word colis (or caulis) meaning a stem, which is also the root of the word cauliflower.\\n\\nThe surname â€œColeâ€ and the variant â€œColesâ€ are fairly common in England and Wales, but are not related to the latin word for cabbage. Both are diminutives of the name â€œNicholasâ€.\\n\\n9. [â€¦] I posted a little piece about Bayesian probability. That one and the others that followed it (here and here) proved to be surprisingly popular so Iâ€™ve been planning to add a few more posts [â€¦]\\n\\n10. It already has a popular name: Stiglerâ€™s law of eponymy.', 'date': '2016-09-28 22:12:05', 'metadata': '{\"extraction_info\": {\"found_math\": true, \"script_math_tex\": 0, \"script_math_asciimath\": 0, \"math_annotations\": 0, \"math_alttext\": 0, \"mathml\": 0, \"mathjax_tag\": 0, \"mathjax_inline_tex\": 0, \"mathjax_display_tex\": 0, \"mathjax_asciimath\": 0, \"img_math\": 4, \"codecogs_latex\": 0, \"wp_latex\": 0, \"mimetex.cgi\": 0, \"/images/math/codecogs\": 0, \"mathtex.cgi\": 0, \"katex\": 0, \"math-container\": 0, \"wp-katex-eq\": 0, \"align\": 0, \"equation\": 0, \"x-ck12\": 0, \"texerror\": 0, \"math_score\": 0.6193313002586365, \"perplexity\": 1089.0032368849284}, \"config\": {\"markdown_headings\": false, \"markdown_code\": true, \"boilerplate_config\": {\"ratio_threshold\": 0.18, \"absolute_threshold\": 10, \"end_threshold\": 15, \"enable\": true}, \"remove_buttons\": true, \"remove_image_figures\": true, \"remove_link_clusters\": true, \"table_config\": {\"min_rows\": 2, \"min_cols\": 3, \"format\": \"plain\"}, \"remove_chinese\": true, \"remove_edit_buttons\": true, \"extract_latex\": true}, \"warc_path\": \"s3://commoncrawl/crawl-data/CC-MAIN-2016-40/segments/1474738661768.10/warc/CC-MAIN-20160924173741-00020-ip-10-143-35-109.ec2.internal.warc.gz\"}'}\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['url', 'text', 'date', 'metadata'],\n",
      "        num_rows: 6315233\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"Sample from the dataset:\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bayes and his\\xa0Theorem\\n\\nMy earlier post on Bayesian probability seems to have generated quite a lot of readers, so this lunchtime I thought Iâ€™d add a little bit of background. The previous discussion started from the result\\n\\n$P(B|AC) = K^{-1}P(B|C)P(A|BC) = K^{-1} P(AB|C)$\\n\\nwhere\\n\\n$K=P(A|C).$\\n\\nAlthough this is called Bayesâ€™ theorem, the general form of it as stated here was actually first written down, not by Bayes but by Laplace. What Bayesâ€™ did was derive the special case of this formula for â€œinvertingâ€ the binomial distribution. This distribution gives the probability of x successes in n independent â€œtrialsâ€ each having the same probability of success, p; each â€œtrialâ€ has only two possible outcomes (â€œsuccessâ€ or â€œfailureâ€). Trials like this are usually called Bernoulli trials, after Daniel Bernoulli. If we ask the question â€œwhat is the probability of exactly x successes from the possible n?â€, the answer is given by the binomial distribution:\\n\\n$P_n(x|n,p)= C(n,x) p^x (1-p)^{n-x}$\\n\\nwhere\\n\\n$C(n,x)= n!/x!(n-x)!$\\n\\nis the number of distinct combinations of x objects that can be drawn from a pool of n.\\n\\nYou can probably see immediately how this arises. The probability of x consecutive successes is p multiplied by itself x times, or px. The probability of (n-x) successive failures is similarly (1-p)n-x. The last two terms basically therefore tell us the probability that we have exactly x successes (since there must be n-x failures). The combinatorial factor in front takes account of the fact that the ordering of successes and failures doesnâ€™t matter.\\n\\nThe binomial distribution applies, for example, to repeated tosses of a coin, in which case p is taken to be 0.5 for a fair coin. A biased coin might have a different value of p, but as long as the tosses are independent the formula still applies. The binomial distribution also applies to problems involving drawing balls from urns: it works exactly if the balls are replaced in the urn after each draw, but it also applies approximately without replacement, as long as the number of draws is much smaller than the number of balls in the urn. I leave it as an exercise to calculate the expectation value of the binomial distribution, but the result is not surprising: E(X)=np. If you toss a fair coin ten times the expectation value for the number of heads is 10 times 0.5, which is five. No surprise there. After another bit of maths, the variance of the distribution can also be found. It is np(1-p).\\n\\nSo this gives us the probability of x given a fixed value of p. Bayes was interested in the inverse of this result, the probability of p given x. In other words, Bayes was interested in the answer to the question â€œIf I perform n independent trials and get x successes, what is the probability distribution of p?â€. This is a classic example of inverse reasoning. He got the correct answer, eventually, but by very convoluted reasoning. In my opinion it is quite difficult to justify the name Bayesâ€™ theorem based on what he actually did, although Laplace did specifically acknowledge this contribution when he derived the general result later, which is no doubt why the theorem is always named in Bayesâ€™ honour.\\n\\nThis is not the only example in science where the wrong personâ€™s name is attached to a result or discovery. In fact, it is almost a law of Nature that any theorem that has a name has the wrong name. I propose that this observation should henceforth be known as Colesâ€™ Law.\\n\\nSo who was the mysterious mathematician behind this result? Thomas Bayes was born in 1702, son of Joshua Bayes, who was a Fellow of the Royal Society (FRS) and one of the very first nonconformist ministers to be ordained in England. Thomas was himself ordained and for a while worked with his father in the Presbyterian Meeting House in Leather Lane, near Holborn in London. In 1720 he was a minister in Tunbridge Wells, in Kent. He retired from the church in 1752 and died in 1761. Thomas Bayes didnâ€™t publish a single paper on mathematics in his own name during his lifetime but despite this was elected a Fellow of the Royal Society (FRS) in 1742. Presumably he had Friends of the Right Sort. He did however write a paper on fluxions in 1736, which was published anonymously. This was probably the grounds on which he was elected an FRS.\\n\\nThe paper containing the theorem that now bears his name was published posthumously in the Philosophical Transactions of the Royal Society of London in 1764.\\n\\nP.S. I understand that the authenticity of the picture is open to question. Whoever it actually is, he looks\\xa0 to me a bit like Laurence Olivierâ€¦\\n\\n11 Responses to â€œBayes and his\\xa0Theoremâ€\\n\\n1. Bryn Jones Says:\\n\\nThe Royal Society is providing free access to electronic versions of its journals until the end of this month. Readers of this blog might like to look at Thomas Bayesâ€™s two posthumous publications in the Philosophical Transactions.\\n\\nThe first is a short paper about series. The other is the paper about statistics communicated by Richard Price. (The statistics paper may be accessible on a long-term basis because it is one of the Royal Societyâ€™s Trailblazing papers the society provides access to as part of its 350th anniversary celebrations.)\\n\\nIncidentally, both Thomas Bayes and Richard Price were buried in the Bunhill Fields Cemetery in London and their tombs can be seen there today.\\n\\n2. Steve Warren Says:\\n\\nYou may be remembered in history as the discoverer of coleslaw, but you werenâ€™t the first.\\n\\nâ€¢ Anton Garrett Says:\\n\\nFor years I thought it was â€œcold slawâ€ because it was served cold. A good job I never asked for warm slaw.\\n\\n3. telescoper Says:\\n\\nMy surname, in Spanish, means â€œCabbagesâ€. So it was probably one of my ancestors who invented the chopped variety.\\n\\n4. Anton Garrett Says:\\n\\nThomas Bayes is now known to have gone to Edinburgh University, where his name appears in the records. He was barred from English universities because his nonconformist family did not have him baptised in the Church of England. (Charles Darwinâ€™s nonconformist family covered their bets by having baby Charles baptised in the CoE, although perhaps they believed it didnâ€™t count as a baptism since Charles had no say in it. Tist is why he was able to go to Christâ€™s College, Cambridge.)\\n\\n5. â€œColeâ€ is an old English word for cabbage, which survives in â€œcole slawâ€. The German word is â€œKohlâ€. (Somehow, I donâ€™t see PM or President Cabbage being a realistic possibility. ðŸ™‚ )\\n\\nNote that Old King Cole is unrelated (etymologically). Of course, this discussion could cause Peter to post a clip of\\n\\nNat â€œKingâ€ Cole\\n(guess what his real surname is).\\n\\nTo remind people to pay attention to spelling when they hear words, weâ€™ll close with the Quote of the Day:\\n\\nItâ€™s important to pay close attention in school. For years I thought that\\nbears masturbated all winter.\\n\\nâ€”Damon R. Milhem\\n\\n6. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\n7. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\nThe first typo was my fault; the extra linebreaks in the second attempt\\n(tested again here) appear to be a new â€œfeatureâ€.\\n\\n8. telescoper Says:\\n\\nThe noun â€œcoleâ€ can be found in English dictionaries as a generic name for plants of the cabbage family. It is related to the German kohl and scottish kail or kale. These are all derived from the latin word colis (or caulis) meaning a stem, which is also the root of the word cauliflower.\\n\\nThe surname â€œColeâ€ and the variant â€œColesâ€ are fairly common in England and Wales, but are not related to the latin word for cabbage. Both are diminutives of the name â€œNicholasâ€.\\n\\n9. [â€¦] I posted a little piece about Bayesian probability. That one and the others that followed it (here and here) proved to be surprisingly popular so Iâ€™ve been planning to add a few more posts [â€¦]\\n\\n10. It already has a popular name: Stiglerâ€™s law of eponymy.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenizer Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Sample text:\n",
      "Bayes and hisÂ Theorem\n",
      "\n",
      "My earlier post on Bayesian probability seems to have generated quite a lot of readers, so this lunchtime I thought Iâ€™d add a little bit of background. The previous discussion started from the result\n",
      "\n",
      "$P(B|AC) = K^{-1}P(B|C)P(A|BC) = K^{-1} P(AB|C)$\n",
      "\n",
      "where\n",
      "\n",
      "$K=P(A|C).$\n",
      "\n",
      "Although this is called Bayesâ€™ theorem, the general form of it as stated here was actually first written down, not by Bayes but by Laplace. What Bayesâ€™ did was derive the special case of this formula for â€œinvertingâ€ the binomial distribution. This distribution gives the probability of x successes in n independent â€œtrialsâ€ each having the same probability of success, p; each â€œtrialâ€ has only two possible outcomes (â€œsuccessâ€ or â€œfailureâ€). Trials like this are usually called Bernoulli trials, after Daniel Bernoulli. If we ask the question â€œwhat is the probability of exactly x successes from the possible n?â€, the answer is given by the binomial distribution:\n",
      "\n",
      "$P_n(x|n,p)= C(n,x) p^x (1-p)^{n-x}$\n",
      "\n",
      "where\n",
      "\n",
      "$C(n,x)= n!/x!(n-x)!$\n",
      "\n",
      "is the number of distinct combinations of x objects that can be drawn from a pool of n.\n",
      "\n",
      "You can probably see immediately how this arises. The probability of x consecutive successes is p multiplied by itself x times, or px. The probability of (n-x) successive failures is similarly (1-p)n-x. The last two terms basically therefore tell us the probability that we have exactly x successes (since there must be n-x failures). The combinatorial factor in front takes account of the fact that the ordering of successes and failures doesnâ€™t matter.\n",
      "\n",
      "The binomial distribution applies, for example, to repeated tosses of a coin, in which case p is taken to be 0.5 for a fair coin. A biased coin might have a different value of p, but as long as the tosses are independent the formula still applies. The binomial distribution also applies to problems involving drawing balls from urns: it works exactly if the balls are replaced in the urn after each draw, but it also applies approximately without replacement, as long as the number of draws is much smaller than the number of balls in the urn. I leave it as an exercise to calculate the expectation value of the binomial distribution, but the result is not surprising: E(X)=np. If you toss a fair coin ten times the expectation value for the number of heads is 10 times 0.5, which is five. No surprise there. After another bit of maths, the variance of the distribution can also be found. It is np(1-p).\n",
      "\n",
      "So this gives us the probability of x given a fixed value of p. Bayes was interested in the inverse of this result, the probability of p given x. In other words, Bayes was interested in the answer to the question â€œIf I perform n independent trials and get x successes, what is the probability distribution of p?â€. This is a classic example of inverse reasoning. He got the correct answer, eventually, but by very convoluted reasoning. In my opinion it is quite difficult to justify the name Bayesâ€™ theorem based on what he actually did, although Laplace did specifically acknowledge this contribution when he derived the general result later, which is no doubt why the theorem is always named in Bayesâ€™ honour.\n",
      "\n",
      "This is not the only example in science where the wrong personâ€™s name is attached to a result or discovery. In fact, it is almost a law of Nature that any theorem that has a name has the wrong name. I propose that this observation should henceforth be known as Colesâ€™ Law.\n",
      "\n",
      "So who was the mysterious mathematician behind this result? Thomas Bayes was born in 1702, son of Joshua Bayes, who was a Fellow of the Royal Society (FRS) and one of the very first nonconformist ministers to be ordained in England. Thomas was himself ordained and for a while worked with his father in the Presbyterian Meeting House in Leather Lane, near Holborn in London. In 1720 he was a minister in Tunbridge Wells, in Kent. He retired from the church in 1752 and died in 1761. Thomas Bayes didnâ€™t publish a single paper on mathematics in his own name during his lifetime but despite this was elected a Fellow of the Royal Society (FRS) in 1742. Presumably he had Friends of the Right Sort. He did however write a paper on fluxions in 1736, which was published anonymously. This was probably the grounds on which he was elected an FRS.\n",
      "\n",
      "The paper containing the theorem that now bears his name was published posthumously in the Philosophical Transactions of the Royal Society of London in 1764.\n",
      "\n",
      "P.S. I understand that the authenticity of the picture is open to question. Whoever it actually is, he looksÂ  to me a bit like Laurence Olivierâ€¦\n",
      "\n",
      "11 Responses to â€œBayes and hisÂ Theoremâ€\n",
      "\n",
      "1. Bryn Jones Says:\n",
      "\n",
      "The Royal Society is providing free access to electronic versions of its journals until the end of this month. Readers of this blog might like to look at Thomas Bayesâ€™s two posthumous publications in the Philosophical Transactions.\n",
      "\n",
      "The first is a short paper about series. The other is the paper about statistics communicated by Richard Price. (The statistics paper may be accessible on a long-term basis because it is one of the Royal Societyâ€™s Trailblazing papers the society provides access to as part of its 350th anniversary celebrations.)\n",
      "\n",
      "Incidentally, both Thomas Bayes and Richard Price were buried in the Bunhill Fields Cemetery in London and their tombs can be seen there today.\n",
      "\n",
      "2. Steve Warren Says:\n",
      "\n",
      "You may be remembered in history as the discoverer of coleslaw, but you werenâ€™t the first.\n",
      "\n",
      "â€¢ Anton Garrett Says:\n",
      "\n",
      "For years I thought it was â€œcold slawâ€ because it was served cold. A good job I never asked for warm slaw.\n",
      "\n",
      "3. telescoper Says:\n",
      "\n",
      "My surname, in Spanish, means â€œCabbagesâ€. So it was probably one of my ancestors who invented the chopped variety.\n",
      "\n",
      "4. Anton Garrett Says:\n",
      "\n",
      "Thomas Bayes is now known to have gone to Edinburgh University, where his name appears in the records. He was barred from English universities because his nonconformist family did not have him baptised in the Church of England. (Charles Darwinâ€™s nonconformist family covered their bets by having baby Charles baptised in the CoE, although perhaps they believed it didnâ€™t count as a baptism since Charles had no say in it. Tist is why he was able to go to Christâ€™s College, Cambridge.)\n",
      "\n",
      "5. â€œColeâ€ is an old English word for cabbage, which survives in â€œcole slawâ€. The German word is â€œKohlâ€. (Somehow, I donâ€™t see PM or President Cabbage being a realistic possibility. ðŸ™‚ )\n",
      "\n",
      "Note that Old King Cole is unrelated (etymologically). Of course, this discussion could cause Peter to post a clip of\n",
      "\n",
      "Nat â€œKingâ€ Cole\n",
      "(guess what his real surname is).\n",
      "\n",
      "To remind people to pay attention to spelling when they hear words, weâ€™ll close with the Quote of the Day:\n",
      "\n",
      "Itâ€™s important to pay close attention in school. For years I thought that\n",
      "bears masturbated all winter.\n",
      "\n",
      "â€”Damon R. Milhem\n",
      "\n",
      "6. Of course, this discussion could cause Peter to post a clip of\n",
      "Nat King Cole\n",
      "(giess what his real surname is).\n",
      "\n",
      "7. Of course, this discussion could cause Peter to post a clip of\n",
      "Nat King Cole\n",
      "(giess what his real surname is).\n",
      "\n",
      "The first typo was my fault; the extra linebreaks in the second attempt\n",
      "(tested again here) appear to be a new â€œfeatureâ€.\n",
      "\n",
      "8. telescoper Says:\n",
      "\n",
      "The noun â€œcoleâ€ can be found in English dictionaries as a generic name for plants of the cabbage family. It is related to the German kohl and scottish kail or kale. These are all derived from the latin word colis (or caulis) meaning a stem, which is also the root of the word cauliflower.\n",
      "\n",
      "The surname â€œColeâ€ and the variant â€œColesâ€ are fairly common in England and Wales, but are not related to the latin word for cabbage. Both are diminutives of the name â€œNicholasâ€.\n",
      "\n",
      "9. [â€¦] I posted a little piece about Bayesian probability. That one and the others that followed it (here and here) proved to be surprisingly popular so Iâ€™ve been planning to add a few more posts [â€¦]\n",
      "\n",
      "10. It already has a popular name: Stiglerâ€™s law of eponymy.\n",
      "Tokens:\n",
      "['Bay', 'es', 'Ä and', 'Ä his', 'Ã‚Å‚', 'The', 'orem', 'ÄŠÄŠ', 'My', 'Ä earlier', 'Ä post', 'Ä on', 'Ä Bayesian', 'Ä probability', 'Ä seems', 'Ä to', 'Ä have', 'Ä generated', 'Ä quite', 'Ä a', 'Ä lot', 'Ä of', 'Ä readers', ',', 'Ä so', 'Ä this', 'Ä lunch', 'time', 'Ä I', 'Ä thought', 'Ä I', 'Ã¢Ä¢Ä»d', 'Ä add', 'Ä a', 'Ä little', 'Ä bit', 'Ä of', 'Ä background', '.', 'Ä The', 'Ä previous', 'Ä discussion', 'Ä started', 'Ä from', 'Ä the', 'Ä result', 'ÄŠÄŠ', '$', 'P', '(B', '|', 'AC', ')', 'Ä =', 'Ä K', '^{-', '1', '}', 'P', '(B', '|', 'C', ')', 'P', '(A', '|', 'BC', ')', 'Ä =', 'Ä K', '^{-', '1', '}', 'Ä P', '(', 'AB', '|', 'C', ')$', 'ÄŠÄŠ', 'where', 'ÄŠÄŠ', '$', 'K', '=P', '(A', '|', 'C', ').', '$ÄŠÄŠ', 'Although', 'Ä this', 'Ä is', 'Ä called', 'Ä Bay', 'es', 'Ã¢Ä¢Ä»', 'Ä theorem', ',', 'Ä the', 'Ä general', 'Ä form', 'Ä of', 'Ä it', 'Ä as', 'Ä stated', 'Ä here', 'Ä was', 'Ä actually', 'Ä first', 'Ä written', 'Ä down', ',', 'Ä not', 'Ä by', 'Ä Bay', 'es', 'Ä but', 'Ä by', 'Ä Lap', 'lace', '.', 'Ä What', 'Ä Bay', 'es', 'Ã¢Ä¢Ä»', 'Ä did', 'Ä was', 'Ä derive', 'Ä the', 'Ä special', 'Ä case', 'Ä of', 'Ä this', 'Ä formula', 'Ä for', 'Ä Ã¢Ä¢Ä¾', 'in', 'verting', 'Ã¢Ä¢Ä¿', 'Ä the', 'Ä bin', 'omial', 'Ä distribution', '.', 'Ä This', 'Ä distribution', 'Ä gives', 'Ä the', 'Ä probability', 'Ä of', 'Ä x', 'Ä successes', 'Ä in', 'Ä n', 'Ä independent', 'Ä Ã¢Ä¢Ä¾', 'tr', 'ials', 'Ã¢Ä¢Ä¿', 'Ä each', 'Ä having', 'Ä the', 'Ä same', 'Ä probability', 'Ä of', 'Ä success', ',', 'Ä p', ';', 'Ä each', 'Ä Ã¢Ä¢Ä¾', 'trial', 'Ã¢Ä¢Ä¿', 'Ä has', 'Ä only', 'Ä two', 'Ä possible', 'Ä outcomes', 'Ä (Ã¢Ä¢Ä¾', 'success', 'Ã¢Ä¢Ä¿', 'Ä or', 'Ä Ã¢Ä¢Ä¾', 'failure', 'Ã¢Ä¢Ä¿).', 'Ä Trials', 'Ä like', 'Ä this', 'Ä are', 'Ä usually', 'Ä called', 'Ä Bern', 'ou', 'lli', 'Ä trials', ',', 'Ä after', 'Ä Daniel', 'Ä Bern', 'ou', 'lli', '.', 'Ä If', 'Ä we', 'Ä ask', 'Ä the', 'Ä question', 'Ä Ã¢Ä¢Ä¾', 'what', 'Ä is', 'Ä the', 'Ä probability', 'Ä of', 'Ä exactly', 'Ä x', 'Ä successes', 'Ä from', 'Ä the', 'Ä possible', 'Ä n', '?', 'Ã¢Ä¢Ä¿,', 'Ä the', 'Ä answer', 'Ä is', 'Ä given', 'Ä by', 'Ä the', 'Ä bin', 'omial', 'Ä distribution', ':ÄŠÄŠ', '$', 'P', '_n', '(x', '|', 'n', ',p', ')=', 'Ä C', '(n', ',x', ')', 'Ä p', '^', 'x', 'Ä (', '1', '-p', ')^', '{', 'n', '-x', '}$', 'ÄŠÄŠ', 'where', 'ÄŠÄŠ', '$', 'C', '(n', ',x', ')=', 'Ä n', '!/', 'x', '!(', 'n', '-x', ')!', '$ÄŠÄŠ', 'is', 'Ä the', 'Ä number', 'Ä of', 'Ä distinct', 'Ä combinations', 'Ä of', 'Ä x', 'Ä objects', 'Ä that', 'Ä can', 'Ä be', 'Ä drawn', 'Ä from', 'Ä a', 'Ä pool', 'Ä of', 'Ä n', '.ÄŠÄŠ', 'You', 'Ä can', 'Ä probably', 'Ä see', 'Ä immediately', 'Ä how', 'Ä this', 'Ä arises', '.', 'Ä The', 'Ä probability', 'Ä of', 'Ä x', 'Ä consecutive', 'Ä successes', 'Ä is', 'Ä p', 'Ä multiplied', 'Ä by', 'Ä itself', 'Ä x', 'Ä times', ',', 'Ä or', 'Ä px', '.', 'Ä The', 'Ä probability', 'Ä of', 'Ä (', 'n', '-x', ')', 'Ä successive', 'Ä failures', 'Ä is', 'Ä similarly', 'Ä (', '1', '-p', ')n', '-x', '.', 'Ä The', 'Ä last', 'Ä two', 'Ä terms', 'Ä basically', 'Ä therefore', 'Ä tell', 'Ä us', 'Ä the', 'Ä probability', 'Ä that', 'Ä we', 'Ä have', 'Ä exactly', 'Ä x', 'Ä successes', 'Ä (', 'since', 'Ä there', 'Ä must', 'Ä be', 'Ä n', '-x', 'Ä failures', ').', 'Ä The', 'Ä comb', 'inator', 'ial', 'Ä factor', 'Ä in', 'Ä front', 'Ä takes', 'Ä account', 'Ä of', 'Ä the', 'Ä fact', 'Ä that', 'Ä the', 'Ä ordering', 'Ä of', 'Ä successes', 'Ä and', 'Ä failures', 'Ä doesn', 'Ã¢Ä¢Ä»t', 'Ä matter', '.ÄŠÄŠ', 'The', 'Ä bin', 'omial', 'Ä distribution', 'Ä applies', ',', 'Ä for', 'Ä example', ',', 'Ä to', 'Ä repeated', 'Ä toss', 'es', 'Ä of', 'Ä a', 'Ä coin', ',', 'Ä in', 'Ä which', 'Ä case', 'Ä p', 'Ä is', 'Ä taken', 'Ä to', 'Ä be', 'Ä ', '0', '.', '5', 'Ä for', 'Ä a', 'Ä fair', 'Ä coin', '.', 'Ä A', 'Ä biased', 'Ä coin', 'Ä might', 'Ä have', 'Ä a', 'Ä different', 'Ä value', 'Ä of', 'Ä p', ',', 'Ä but', 'Ä as', 'Ä long', 'Ä as', 'Ä the', 'Ä toss', 'es', 'Ä are', 'Ä independent', 'Ä the', 'Ä formula', 'Ä still', 'Ä applies', '.', 'Ä The', 'Ä bin', 'omial', 'Ä distribution', 'Ä also', 'Ä applies', 'Ä to', 'Ä problems', 'Ä involving', 'Ä drawing', 'Ä balls', 'Ä from', 'Ä urn', 's', ':', 'Ä it', 'Ä works', 'Ä exactly', 'Ä if', 'Ä the', 'Ä balls', 'Ä are', 'Ä replaced', 'Ä in', 'Ä the', 'Ä urn', 'Ä after', 'Ä each', 'Ä draw', ',', 'Ä but', 'Ä it', 'Ä also', 'Ä applies', 'Ä approximately', 'Ä without', 'Ä replacement', ',', 'Ä as', 'Ä long', 'Ä as', 'Ä the', 'Ä number', 'Ä of', 'Ä draws', 'Ä is', 'Ä much', 'Ä smaller', 'Ä than', 'Ä the', 'Ä number', 'Ä of', 'Ä balls', 'Ä in', 'Ä the', 'Ä urn', '.', 'Ä I', 'Ä leave', 'Ä it', 'Ä as', 'Ä an', 'Ä exercise', 'Ä to', 'Ä calculate', 'Ä the', 'Ä expectation', 'Ä value', 'Ä of', 'Ä the', 'Ä bin', 'omial', 'Ä distribution', ',', 'Ä but', 'Ä the', 'Ä result', 'Ä is', 'Ä not', 'Ä surprising', ':', 'Ä E', '(X', ')=', 'np', '.', 'Ä If', 'Ä you', 'Ä toss', 'Ä a', 'Ä fair', 'Ä coin', 'Ä ten', 'Ä times', 'Ä the', 'Ä expectation', 'Ä value', 'Ä for', 'Ä the', 'Ä number', 'Ä of', 'Ä heads', 'Ä is', 'Ä ', '10', 'Ä times', 'Ä ', '0', '.', '5', ',', 'Ä which', 'Ä is', 'Ä five', '.', 'Ä No', 'Ä surprise', 'Ä there', '.', 'Ä After', 'Ä another', 'Ä bit', 'Ä of', 'Ä maths', ',', 'Ä the', 'Ä variance', 'Ä of', 'Ä the', 'Ä distribution', 'Ä can', 'Ä also', 'Ä be', 'Ä found', '.', 'Ä It', 'Ä is', 'Ä np', '(', '1', '-p', ').ÄŠÄŠ', 'So', 'Ä this', 'Ä gives', 'Ä us', 'Ä the', 'Ä probability', 'Ä of', 'Ä x', 'Ä given', 'Ä a', 'Ä fixed', 'Ä value', 'Ä of', 'Ä p', '.', 'Ä Bay', 'es', 'Ä was', 'Ä interested', 'Ä in', 'Ä the', 'Ä inverse', 'Ä of', 'Ä this', 'Ä result', ',', 'Ä the', 'Ä probability', 'Ä of', 'Ä p', 'Ä given', 'Ä x', '.', 'Ä In', 'Ä other', 'Ä words', ',', 'Ä Bay', 'es', 'Ä was', 'Ä interested', 'Ä in', 'Ä the', 'Ä answer', 'Ä to', 'Ä the', 'Ä question', 'Ä Ã¢Ä¢Ä¾', 'If', 'Ä I', 'Ä perform', 'Ä n', 'Ä independent', 'Ä trials', 'Ä and', 'Ä get', 'Ä x', 'Ä successes', ',', 'Ä what', 'Ä is', 'Ä the', 'Ä probability', 'Ä distribution', 'Ä of', 'Ä p', '?', 'Ã¢Ä¢Ä¿.', 'Ä This', 'Ä is', 'Ä a', 'Ä classic', 'Ä example', 'Ä of', 'Ä inverse', 'Ä reasoning', '.', 'Ä He', 'Ä got', 'Ä the', 'Ä correct', 'Ä answer', ',', 'Ä eventually', ',', 'Ä but', 'Ä by', 'Ä very', 'Ä conv', 'ol', 'uted', 'Ä reasoning', '.', 'Ä In', 'Ä my', 'Ä opinion', 'Ä it', 'Ä is', 'Ä quite', 'Ä difficult', 'Ä to', 'Ä justify', 'Ä the', 'Ä name', 'Ä Bay', 'es', 'Ã¢Ä¢Ä»', 'Ä theorem', 'Ä based', 'Ä on', 'Ä what', 'Ä he', 'Ä actually', 'Ä did', ',', 'Ä although', 'Ä Lap', 'lace', 'Ä did', 'Ä specifically', 'Ä acknowledge', 'Ä this', 'Ä contribution', 'Ä when', 'Ä he', 'Ä derived', 'Ä the', 'Ä general', 'Ä result', 'Ä later', ',', 'Ä which', 'Ä is', 'Ä no', 'Ä doubt', 'Ä why', 'Ä the', 'Ä theorem', 'Ä is', 'Ä always', 'Ä named', 'Ä in', 'Ä Bay', 'es', 'Ã¢Ä¢Ä»', 'Ä honour', '.ÄŠÄŠ', 'This', 'Ä is', 'Ä not', 'Ä the', 'Ä only', 'Ä example', 'Ä in', 'Ä science', 'Ä where', 'Ä the', 'Ä wrong', 'Ä person', 'Ã¢Ä¢Ä»s', 'Ä name', 'Ä is', 'Ä attached', 'Ä to', 'Ä a', 'Ä result', 'Ä or', 'Ä discovery', '.', 'Ä In', 'Ä fact', ',', 'Ä it', 'Ä is', 'Ä almost', 'Ä a', 'Ä law', 'Ä of', 'Ä Nature', 'Ä that', 'Ä any', 'Ä theorem', 'Ä that', 'Ä has', 'Ä a', 'Ä name', 'Ä has', 'Ä the', 'Ä wrong', 'Ä name', '.', 'Ä I', 'Ä propose', 'Ä that', 'Ä this', 'Ä observation', 'Ä should', 'Ä hence', 'forth', 'Ä be', 'Ä known', 'Ä as', 'Ä Co', 'les', 'Ã¢Ä¢Ä»', 'Ä Law', '.ÄŠÄŠ', 'So', 'Ä who', 'Ä was', 'Ä the', 'Ä mysterious', 'Ä mathematic', 'ian', 'Ä behind', 'Ä this', 'Ä result', '?', 'Ä Thomas', 'Ä Bay', 'es', 'Ä was', 'Ä born', 'Ä in', 'Ä ', '170', '2', ',', 'Ä son', 'Ä of', 'Ä Joshua', 'Ä Bay', 'es', ',', 'Ä who', 'Ä was', 'Ä a', 'Ä Fellow', 'Ä of', 'Ä the', 'Ä Royal', 'Ä Society', 'Ä (', 'FR', 'S', ')', 'Ä and', 'Ä one', 'Ä of', 'Ä the', 'Ä very', 'Ä first', 'Ä non', 'con', 'form', 'ist', 'Ä ministers', 'Ä to', 'Ä be', 'Ä ordained', 'Ä in', 'Ä England', '.', 'Ä Thomas', 'Ä was', 'Ä himself', 'Ä ordained', 'Ä and', 'Ä for', 'Ä a', 'Ä while', 'Ä worked', 'Ä with', 'Ä his', 'Ä father', 'Ä in', 'Ä the', 'Ä Presbyterian', 'Ä Meeting', 'Ä House', 'Ä in', 'Ä Leather', 'Ä Lane', ',', 'Ä near', 'Ä Hol', 'born', 'Ä in', 'Ä London', '.', 'Ä In', 'Ä ', '172', '0', 'Ä he', 'Ä was', 'Ä a', 'Ä minister', 'Ä in', 'Ä Tun', 'bridge', 'Ä Wells', ',', 'Ä in', 'Ä Kent', '.', 'Ä He', 'Ä retired', 'Ä from', 'Ä the', 'Ä church', 'Ä in', 'Ä ', '175', '2', 'Ä and', 'Ä died', 'Ä in', 'Ä ', '176', '1', '.', 'Ä Thomas', 'Ä Bay', 'es', 'Ä didn', 'Ã¢Ä¢Ä»t', 'Ä publish', 'Ä a', 'Ä single', 'Ä paper', 'Ä on', 'Ä mathematics', 'Ä in', 'Ä his', 'Ä own', 'Ä name', 'Ä during', 'Ä his', 'Ä lifetime', 'Ä but', 'Ä despite', 'Ä this', 'Ä was', 'Ä elected', 'Ä a', 'Ä Fellow', 'Ä of', 'Ä the', 'Ä Royal', 'Ä Society', 'Ä (', 'FR', 'S', ')', 'Ä in', 'Ä ', '174', '2', '.', 'Ä Pres', 'umably', 'Ä he', 'Ä had', 'Ä Friends', 'Ä of', 'Ä the', 'Ä Right', 'Ä Sort', '.', 'Ä He', 'Ä did', 'Ä however', 'Ä write', 'Ä a', 'Ä paper', 'Ä on', 'Ä flux', 'ions', 'Ä in', 'Ä ', '173', '6', ',', 'Ä which', 'Ä was', 'Ä published', 'Ä anonymously', '.', 'Ä This', 'Ä was', 'Ä probably', 'Ä the', 'Ä grounds', 'Ä on', 'Ä which', 'Ä he', 'Ä was', 'Ä elected', 'Ä an', 'Ä F', 'RS', '.ÄŠÄŠ', 'The', 'Ä paper', 'Ä containing', 'Ä the', 'Ä theorem', 'Ä that', 'Ä now', 'Ä bears', 'Ä his', 'Ä name', 'Ä was', 'Ä published', 'Ä post', 'hum', 'ously', 'Ä in', 'Ä the', 'Ä Philosoph', 'ical', 'Ä Transactions', 'Ä of', 'Ä the', 'Ä Royal', 'Ä Society', 'Ä of', 'Ä London', 'Ä in', 'Ä ', '176', '4', '.ÄŠÄŠ', 'P', '.S', '.', 'Ä I', 'Ä understand', 'Ä that', 'Ä the', 'Ä authenticity', 'Ä of', 'Ä the', 'Ä picture', 'Ä is', 'Ä open', 'Ä to', 'Ä question', '.', 'Ä Whoever', 'Ä it', 'Ä actually', 'Ä is', ',', 'Ä he', 'Ä looks', 'Ã‚Å‚', 'Ä to', 'Ä me', 'Ä a', 'Ä bit', 'Ä like', 'Ä Laure', 'nce', 'Ä Olivier', 'Ã¢Ä¢Â¦ÄŠÄŠ', '11', 'Ä Responses', 'Ä to', 'Ä Ã¢Ä¢Ä¾', 'Bay', 'es', 'Ä and', 'Ä his', 'Ã‚Å‚', 'The', 'orem', 'Ã¢Ä¢Ä¿ÄŠÄŠ', '1', '.', 'Ä Bry', 'n', 'Ä Jones', 'Ä Says', ':ÄŠÄŠ', 'The', 'Ä Royal', 'Ä Society', 'Ä is', 'Ä providing', 'Ä free', 'Ä access', 'Ä to', 'Ä electronic', 'Ä versions', 'Ä of', 'Ä its', 'Ä journals', 'Ä until', 'Ä the', 'Ä end', 'Ä of', 'Ä this', 'Ä month', '.', 'Ä Readers', 'Ä of', 'Ä this', 'Ä blog', 'Ä might', 'Ä like', 'Ä to', 'Ä look', 'Ä at', 'Ä Thomas', 'Ä Bay', 'es', 'Ã¢Ä¢Ä»s', 'Ä two', 'Ä post', 'hum', 'ous', 'Ä publications', 'Ä in', 'Ä the', 'Ä Philosoph', 'ical', 'Ä Transactions', '.ÄŠÄŠ', 'The', 'Ä first', 'Ä is', 'Ä a', 'Ä short', 'Ä paper', 'Ä about', 'Ä series', '.', 'Ä The', 'Ä other', 'Ä is', 'Ä the', 'Ä paper', 'Ä about', 'Ä statistics', 'Ä communicated', 'Ä by', 'Ä Richard', 'Ä Price', '.', 'Ä (', 'The', 'Ä statistics', 'Ä paper', 'Ä may', 'Ä be', 'Ä accessible', 'Ä on', 'Ä a', 'Ä long', '-term', 'Ä basis', 'Ä because', 'Ä it', 'Ä is', 'Ä one', 'Ä of', 'Ä the', 'Ä Royal', 'Ä Society', 'Ã¢Ä¢Ä»s', 'Ä Trail', 'bl', 'azing', 'Ä papers', 'Ä the', 'Ä society', 'Ä provides', 'Ä access', 'Ä to', 'Ä as', 'Ä part', 'Ä of', 'Ä its', 'Ä ', '350', 'th', 'Ä anniversary', 'Ä celebrations', '.)ÄŠÄŠ', 'Inc', 'identally', ',', 'Ä both', 'Ä Thomas', 'Ä Bay', 'es', 'Ä and', 'Ä Richard', 'Ä Price', 'Ä were', 'Ä buried', 'Ä in', 'Ä the', 'Ä Bun', 'hill', 'Ä Fields', 'Ä Cemetery', 'Ä in', 'Ä London', 'Ä and', 'Ä their', 'Ä tom', 'bs', 'Ä can', 'Ä be', 'Ä seen', 'Ä there', 'Ä today', '.ÄŠÄŠ', '2', '.', 'Ä Steve', 'Ä Warren', 'Ä Says', ':ÄŠÄŠ', 'You', 'Ä may', 'Ä be', 'Ä remembered', 'Ä in', 'Ä history', 'Ä as', 'Ä the', 'Ä discover', 'er', 'Ä of', 'Ä co', 'les', 'law', ',', 'Ä but', 'Ä you', 'Ä weren', 'Ã¢Ä¢Ä»t', 'Ä the', 'Ä first', '.ÄŠÄŠ', 'Ã¢Ä¢Â¢', 'Ä Anton', 'Ä Garrett', 'Ä Says', ':ÄŠÄŠ', 'For', 'Ä years', 'Ä I', 'Ä thought', 'Ä it', 'Ä was', 'Ä Ã¢Ä¢Ä¾', 'cold', 'Ä sl', 'aw', 'Ã¢Ä¢Ä¿', 'Ä because', 'Ä it', 'Ä was', 'Ä served', 'Ä cold', '.', 'Ä A', 'Ä good', 'Ä job', 'Ä I', 'Ä never', 'Ä asked', 'Ä for', 'Ä warm', 'Ä sl', 'aw', '.ÄŠÄŠ', '3', '.', 'Ä telesc', 'oper', 'Ä Says', ':ÄŠÄŠ', 'My', 'Ä surname', ',', 'Ä in', 'Ä Spanish', ',', 'Ä means', 'Ä Ã¢Ä¢Ä¾', 'C', 'abb', 'ages', 'Ã¢Ä¢Ä¿.', 'Ä So', 'Ä it', 'Ä was', 'Ä probably', 'Ä one', 'Ä of', 'Ä my', 'Ä ancestors', 'Ä who', 'Ä invented', 'Ä the', 'Ä chopped', 'Ä variety', '.ÄŠÄŠ', '4', '.', 'Ä Anton', 'Ä Garrett', 'Ä Says', ':ÄŠÄŠ', 'Thomas', 'Ä Bay', 'es', 'Ä is', 'Ä now', 'Ä known', 'Ä to', 'Ä have', 'Ä gone', 'Ä to', 'Ä Edinburgh', 'Ä University', ',', 'Ä where', 'Ä his', 'Ä name', 'Ä appears', 'Ä in', 'Ä the', 'Ä records', '.', 'Ä He', 'Ä was', 'Ä barred', 'Ä from', 'Ä English', 'Ä universities', 'Ä because', 'Ä his', 'Ä non', 'con', 'form', 'ist', 'Ä family', 'Ä did', 'Ä not', 'Ä have', 'Ä him', 'Ä bapt', 'ised', 'Ä in', 'Ä the', 'Ä Church', 'Ä of', 'Ä England', '.', 'Ä (', 'Charles', 'Ä Darwin', 'Ã¢Ä¢Ä»s', 'Ä non', 'con', 'form', 'ist', 'Ä family', 'Ä covered', 'Ä their', 'Ä bets', 'Ä by', 'Ä having', 'Ä baby', 'Ä Charles', 'Ä bapt', 'ised', 'Ä in', 'Ä the', 'Ä Co', 'E', ',', 'Ä although', 'Ä perhaps', 'Ä they', 'Ä believed', 'Ä it', 'Ä didn', 'Ã¢Ä¢Ä»t', 'Ä count', 'Ä as', 'Ä a', 'Ä baptism', 'Ä since', 'Ä Charles', 'Ä had', 'Ä no', 'Ä say', 'Ä in', 'Ä it', '.', 'Ä T', 'ist', 'Ä is', 'Ä why', 'Ä he', 'Ä was', 'Ä able', 'Ä to', 'Ä go', 'Ä to', 'Ä Christ', 'Ã¢Ä¢Ä»s', 'Ä College', ',', 'Ä Cambridge', '.)ÄŠÄŠ', '5', '.', 'Ä Ã¢Ä¢Ä¾', 'Cole', 'Ã¢Ä¢Ä¿', 'Ä is', 'Ä an', 'Ä old', 'Ä English', 'Ä word', 'Ä for', 'Ä cabbage', ',', 'Ä which', 'Ä survives', 'Ä in', 'Ä Ã¢Ä¢Ä¾', 'cole', 'Ä sl', 'aw', 'Ã¢Ä¢Ä¿.', 'Ä The', 'Ä German', 'Ä word', 'Ä is', 'Ä Ã¢Ä¢Ä¾', 'K', 'ohl', 'Ã¢Ä¢Ä¿.', 'Ä (', 'Some', 'how', ',', 'Ä I', 'Ä don', 'Ã¢Ä¢Ä»t', 'Ä see', 'Ä PM', 'Ä or', 'Ä President', 'Ä C', 'abbage', 'Ä being', 'Ä a', 'Ä realistic', 'Ä possibility', '.', 'Ä Ã°ÅÄ»Ä¤', 'Ä )ÄŠÄŠ', 'Note', 'Ä that', 'Ä Old', 'Ä King', 'Ä Cole', 'Ä is', 'Ä unrelated', 'Ä (', 'et', 'ym', 'ologically', ').', 'Ä Of', 'Ä course', ',', 'Ä this', 'Ä discussion', 'Ä could', 'Ä cause', 'Ä Peter', 'Ä to', 'Ä post', 'Ä a', 'Ä clip', 'Ä of', 'ÄŠÄŠ', 'Nat', 'Ä Ã¢Ä¢Ä¾', 'King', 'Ã¢Ä¢Ä¿', 'Ä Cole', 'ÄŠ', '(guess', 'Ä what', 'Ä his', 'Ä real', 'Ä surname', 'Ä is', ').ÄŠÄŠ', 'To', 'Ä remind', 'Ä people', 'Ä to', 'Ä pay', 'Ä attention', 'Ä to', 'Ä spelling', 'Ä when', 'Ä they', 'Ä hear', 'Ä words', ',', 'Ä we', 'Ã¢Ä¢Ä»ll', 'Ä close', 'Ä with', 'Ä the', 'Ä Quote', 'Ä of', 'Ä the', 'Ä Day', ':ÄŠÄŠ', 'It', 'Ã¢Ä¢Ä»s', 'Ä important', 'Ä to', 'Ä pay', 'Ä close', 'Ä attention', 'Ä in', 'Ä school', '.', 'Ä For', 'Ä years', 'Ä I', 'Ä thought', 'Ä that', 'ÄŠ', 'b', 'ears', 'Ä masturb', 'ated', 'Ä all', 'Ä winter', '.ÄŠÄŠ', 'Ã¢Ä¢Ä¶', 'D', 'amon', 'Ä R', '.', 'Ä Mil', 'hem', 'ÄŠÄŠ', '6', '.', 'Ä Of', 'Ä course', ',', 'Ä this', 'Ä discussion', 'Ä could', 'Ä cause', 'Ä Peter', 'Ä to', 'Ä post', 'Ä a', 'Ä clip', 'Ä of', 'ÄŠ', 'Nat', 'Ä King', 'Ä Cole', 'ÄŠ', '(g', 'i', 'ess', 'Ä what', 'Ä his', 'Ä real', 'Ä surname', 'Ä is', ').ÄŠÄŠ', '7', '.', 'Ä Of', 'Ä course', ',', 'Ä this', 'Ä discussion', 'Ä could', 'Ä cause', 'Ä Peter', 'Ä to', 'Ä post', 'Ä a', 'Ä clip', 'Ä of', 'ÄŠ', 'Nat', 'Ä King', 'Ä Cole', 'ÄŠ', '(g', 'i', 'ess', 'Ä what', 'Ä his', 'Ä real', 'Ä surname', 'Ä is', ').ÄŠÄŠ', 'The', 'Ä first', 'Ä typo', 'Ä was', 'Ä my', 'Ä fault', ';', 'Ä the', 'Ä extra', 'Ä line', 'break', 's', 'Ä in', 'Ä the', 'Ä second', 'Ä attempt', 'ÄŠ', '(test', 'ed', 'Ä again', 'Ä here', ')', 'Ä appear', 'Ä to', 'Ä be', 'Ä a', 'Ä new', 'Ä Ã¢Ä¢Ä¾', 'feature', 'Ã¢Ä¢Ä¿.ÄŠÄŠ', '8', '.', 'Ä telesc', 'oper', 'Ä Says', ':ÄŠÄŠ', 'The', 'Ä noun', 'Ä Ã¢Ä¢Ä¾', 'cole', 'Ã¢Ä¢Ä¿', 'Ä can', 'Ä be', 'Ä found', 'Ä in', 'Ä English', 'Ä dictionaries', 'Ä as', 'Ä a', 'Ä generic', 'Ä name', 'Ä for', 'Ä plants', 'Ä of', 'Ä the', 'Ä cabbage', 'Ä family', '.', 'Ä It', 'Ä is', 'Ä related', 'Ä to', 'Ä the', 'Ä German', 'Ä k', 'ohl', 'Ä and', 'Ä sc', 'ott', 'ish', 'Ä k', 'ail', 'Ä or', 'Ä kale', '.', 'Ä These', 'Ä are', 'Ä all', 'Ä derived', 'Ä from', 'Ä the', 'Ä latin', 'Ä word', 'Ä col', 'is', 'Ä (', 'or', 'Ä ca', 'ulis', ')', 'Ä meaning', 'Ä a', 'Ä stem', ',', 'Ä which', 'Ä is', 'Ä also', 'Ä the', 'Ä root', 'Ä of', 'Ä the', 'Ä word', 'Ä cauliflower', '.ÄŠÄŠ', 'The', 'Ä surname', 'Ä Ã¢Ä¢Ä¾', 'Cole', 'Ã¢Ä¢Ä¿', 'Ä and', 'Ä the', 'Ä variant', 'Ä Ã¢Ä¢Ä¾', 'C', 'oles', 'Ã¢Ä¢Ä¿', 'Ä are', 'Ä fairly', 'Ä common', 'Ä in', 'Ä England', 'Ä and', 'Ä Wales', ',', 'Ä but', 'Ä are', 'Ä not', 'Ä related', 'Ä to', 'Ä the', 'Ä latin', 'Ä word', 'Ä for', 'Ä cabbage', '.', 'Ä Both', 'Ä are', 'Ä dimin', 'ut', 'ives', 'Ä of', 'Ä the', 'Ä name', 'Ä Ã¢Ä¢Ä¾', 'Nich', 'olas', 'Ã¢Ä¢Ä¿.ÄŠÄŠ', '9', '.', 'Ä [Ã¢Ä¢Â¦]', 'Ä I', 'Ä posted', 'Ä a', 'Ä little', 'Ä piece', 'Ä about', 'Ä Bayesian', 'Ä probability', '.', 'Ä That', 'Ä one', 'Ä and', 'Ä the', 'Ä others', 'Ä that', 'Ä followed', 'Ä it', 'Ä (', 'here', 'Ä and', 'Ä here', ')', 'Ä proved', 'Ä to', 'Ä be', 'Ä surprisingly', 'Ä popular', 'Ä so', 'Ä I', 'Ã¢Ä¢Ä»ve', 'Ä been', 'Ä planning', 'Ä to', 'Ä add', 'Ä a', 'Ä few', 'Ä more', 'Ä posts', 'Ä [Ã¢Ä¢Â¦]ÄŠÄŠ', '10', '.', 'Ä It', 'Ä already', 'Ä has', 'Ä a', 'Ä popular', 'Ä name', ':', 'Ä St', 'ig', 'ler', 'Ã¢Ä¢Ä»s', 'Ä law', 'Ä of', 'Ä e', 'pon', 'ym', 'y', '.']\n",
      "Token IDs:\n",
      "[23407, 288, 323, 813, 4194, 791, 13475, 271, 5159, 6931, 1772, 389, 99234, 19463, 5084, 311, 617, 8066, 5115, 264, 2763, 315, 13016, 11, 779, 420, 16163, 1712, 358, 3463, 358, 7070, 923, 264, 2697, 2766, 315, 4092, 13, 578, 3766, 10430, 3940, 505, 279, 1121, 271, 3, 47, 5462, 91, 1741, 8, 284, 735, 88310, 16, 92, 47, 5462, 91, 34, 8, 47, 4444, 91, 5002, 8, 284, 735, 88310, 16, 92, 393, 7, 1905, 91, 34, 15437, 271, 2940, 271, 3, 42, 77878, 4444, 91, 34, 570, 67526, 16179, 420, 374, 2663, 9332, 288, 529, 58917, 11, 279, 4689, 1376, 315, 433, 439, 11224, 1618, 574, 3604, 1176, 5439, 1523, 11, 539, 555, 9332, 288, 719, 555, 50155, 27634, 13, 3639, 9332, 288, 529, 1550, 574, 43530, 279, 3361, 1162, 315, 420, 15150, 369, 1054, 258, 50517, 863, 279, 9736, 21524, 8141, 13, 1115, 8141, 6835, 279, 19463, 315, 865, 48188, 304, 308, 9678, 1054, 376, 10522, 863, 1855, 3515, 279, 1890, 19463, 315, 2450, 11, 281, 26, 1855, 1054, 48447, 863, 706, 1193, 1403, 3284, 20124, 27179, 5748, 863, 477, 1054, 30039, 65312, 70544, 1093, 420, 527, 6118, 2663, 14502, 283, 41076, 19622, 11, 1306, 15469, 14502, 283, 41076, 13, 1442, 584, 2610, 279, 3488, 1054, 12840, 374, 279, 19463, 315, 7041, 865, 48188, 505, 279, 3284, 308, 30, 9520, 279, 4320, 374, 2728, 555, 279, 9736, 21524, 8141, 1473, 3, 47, 1107, 2120, 91, 77, 7385, 11992, 356, 1471, 13095, 8, 281, 61, 87, 320, 16, 2320, 30876, 90, 77, 6695, 32816, 271, 2940, 271, 3, 34, 1471, 13095, 11992, 308, 89080, 87, 10509, 77, 6695, 42395, 67526, 285, 279, 1396, 315, 12742, 28559, 315, 865, 6302, 430, 649, 387, 15107, 505, 264, 7463, 315, 308, 382, 2675, 649, 4762, 1518, 7214, 1268, 420, 48282, 13, 578, 19463, 315, 865, 24871, 48188, 374, 281, 56016, 555, 5196, 865, 3115, 11, 477, 17585, 13, 578, 19463, 315, 320, 77, 6695, 8, 50024, 28950, 374, 30293, 320, 16, 2320, 80198, 6695, 13, 578, 1566, 1403, 3878, 13524, 9093, 3371, 603, 279, 19463, 430, 584, 617, 7041, 865, 48188, 320, 11536, 1070, 2011, 387, 308, 6695, 28950, 570, 578, 3698, 17720, 532, 8331, 304, 4156, 5097, 2759, 315, 279, 2144, 430, 279, 22106, 315, 48188, 323, 28950, 3250, 1431, 5030, 382, 791, 9736, 21524, 8141, 17208, 11, 369, 3187, 11, 311, 11763, 26251, 288, 315, 264, 16652, 11, 304, 902, 1162, 281, 374, 4529, 311, 387, 220, 15, 13, 20, 369, 264, 6762, 16652, 13, 362, 48761, 16652, 2643, 617, 264, 2204, 907, 315, 281, 11, 719, 439, 1317, 439, 279, 26251, 288, 527, 9678, 279, 15150, 2103, 17208, 13, 578, 9736, 21524, 8141, 1101, 17208, 311, 5435, 16239, 13633, 20953, 505, 66967, 82, 25, 433, 4375, 7041, 422, 279, 20953, 527, 12860, 304, 279, 66967, 1306, 1855, 4128, 11, 719, 433, 1101, 17208, 13489, 2085, 14039, 11, 439, 1317, 439, 279, 1396, 315, 27741, 374, 1790, 9333, 1109, 279, 1396, 315, 20953, 304, 279, 66967, 13, 358, 5387, 433, 439, 459, 10368, 311, 11294, 279, 31293, 907, 315, 279, 9736, 21524, 8141, 11, 719, 279, 1121, 374, 539, 15206, 25, 469, 7799, 11992, 6331, 13, 1442, 499, 26251, 264, 6762, 16652, 5899, 3115, 279, 31293, 907, 369, 279, 1396, 315, 14971, 374, 220, 605, 3115, 220, 15, 13, 20, 11, 902, 374, 4330, 13, 2360, 13051, 1070, 13, 4740, 2500, 2766, 315, 71808, 11, 279, 33373, 315, 279, 8141, 649, 1101, 387, 1766, 13, 1102, 374, 2660, 7, 16, 2320, 3677, 4516, 420, 6835, 603, 279, 19463, 315, 865, 2728, 264, 8521, 907, 315, 281, 13, 9332, 288, 574, 8173, 304, 279, 29049, 315, 420, 1121, 11, 279, 19463, 315, 281, 2728, 865, 13, 763, 1023, 4339, 11, 9332, 288, 574, 8173, 304, 279, 4320, 311, 279, 3488, 1054, 2746, 358, 2804, 308, 9678, 19622, 323, 636, 865, 48188, 11, 1148, 374, 279, 19463, 8141, 315, 281, 30, 11453, 1115, 374, 264, 11670, 3187, 315, 29049, 33811, 13, 1283, 2751, 279, 4495, 4320, 11, 9778, 11, 719, 555, 1633, 5804, 337, 2844, 33811, 13, 763, 856, 9647, 433, 374, 5115, 5107, 311, 9541, 279, 836, 9332, 288, 529, 58917, 3196, 389, 1148, 568, 3604, 1550, 11, 8051, 50155, 27634, 1550, 11951, 25670, 420, 19035, 994, 568, 14592, 279, 4689, 1121, 3010, 11, 902, 374, 912, 10712, 3249, 279, 58917, 374, 2744, 7086, 304, 9332, 288, 529, 34662, 382, 2028, 374, 539, 279, 1193, 3187, 304, 8198, 1405, 279, 5076, 1732, 753, 836, 374, 12673, 311, 264, 1121, 477, 18841, 13, 763, 2144, 11, 433, 374, 4661, 264, 2383, 315, 22037, 430, 904, 58917, 430, 706, 264, 836, 706, 279, 5076, 836, 13, 358, 30714, 430, 420, 22695, 1288, 16472, 71627, 387, 3967, 439, 3623, 645, 529, 7658, 382, 4516, 889, 574, 279, 26454, 21651, 1122, 4920, 420, 1121, 30, 11355, 9332, 288, 574, 9405, 304, 220, 8258, 17, 11, 4538, 315, 40592, 9332, 288, 11, 889, 574, 264, 37946, 315, 279, 16591, 13581, 320, 10725, 50, 8, 323, 832, 315, 279, 1633, 1176, 2536, 444, 630, 380, 35050, 311, 387, 86724, 304, 9635, 13, 11355, 574, 5678, 86724, 323, 369, 264, 1418, 6575, 449, 813, 7126, 304, 279, 86464, 30155, 4783, 304, 32178, 27109, 11, 3221, 16071, 16381, 304, 7295, 13, 763, 220, 10861, 15, 568, 574, 264, 13015, 304, 28556, 14024, 37958, 11, 304, 18206, 13, 1283, 22311, 505, 279, 8993, 304, 220, 10005, 17, 323, 8636, 304, 220, 10967, 16, 13, 11355, 9332, 288, 3287, 1431, 3498, 264, 3254, 5684, 389, 38696, 304, 813, 1866, 836, 2391, 813, 19569, 719, 8994, 420, 574, 16689, 264, 37946, 315, 279, 16591, 13581, 320, 10725, 50, 8, 304, 220, 11771, 17, 13, 4203, 76051, 568, 1047, 23323, 315, 279, 10291, 16347, 13, 1283, 1550, 4869, 3350, 264, 5684, 389, 31405, 919, 304, 220, 11908, 21, 11, 902, 574, 4756, 73251, 13, 1115, 574, 4762, 279, 21319, 389, 902, 568, 574, 16689, 459, 435, 11706, 382, 791, 5684, 8649, 279, 58917, 430, 1457, 30824, 813, 836, 574, 4756, 1772, 28400, 7162, 304, 279, 38356, 950, 56385, 315, 279, 16591, 13581, 315, 7295, 304, 220, 10967, 19, 382, 47, 815, 13, 358, 3619, 430, 279, 54348, 315, 279, 6945, 374, 1825, 311, 3488, 13, 91395, 433, 3604, 374, 11, 568, 5992, 4194, 311, 757, 264, 2766, 1093, 33767, 16848, 78118, 5551, 806, 81567, 311, 1054, 23407, 288, 323, 813, 4194, 791, 13475, 7663, 16, 13, 19803, 77, 12201, 47559, 1473, 791, 16591, 13581, 374, 8405, 1949, 2680, 311, 14683, 11028, 315, 1202, 42780, 3156, 279, 842, 315, 420, 2305, 13, 65514, 315, 420, 5117, 2643, 1093, 311, 1427, 520, 11355, 9332, 288, 753, 1403, 1772, 28400, 788, 29085, 304, 279, 38356, 950, 56385, 382, 791, 1176, 374, 264, 2875, 5684, 922, 4101, 13, 578, 1023, 374, 279, 5684, 922, 13443, 59035, 555, 12131, 8650, 13, 320, 791, 13443, 5684, 1253, 387, 15987, 389, 264, 1317, 9860, 8197, 1606, 433, 374, 832, 315, 279, 16591, 13581, 753, 19177, 2067, 6795, 16064, 279, 8396, 5825, 2680, 311, 439, 961, 315, 1202, 220, 8652, 339, 22310, 47674, 9456, 40345, 57222, 11, 2225, 11355, 9332, 288, 323, 12131, 8650, 1051, 28016, 304, 279, 66347, 28607, 25599, 59578, 304, 7295, 323, 872, 10390, 1302, 649, 387, 3970, 1070, 3432, 382, 17, 13, 14129, 26713, 47559, 1473, 2675, 1253, 387, 27569, 304, 3925, 439, 279, 7142, 261, 315, 1080, 645, 20510, 11, 719, 499, 15058, 1431, 279, 1176, 382, 6806, 17958, 58716, 47559, 1473, 2520, 1667, 358, 3463, 433, 574, 1054, 88172, 1776, 675, 863, 1606, 433, 574, 10434, 9439, 13, 362, 1695, 2683, 358, 2646, 4691, 369, 8369, 1776, 675, 382, 18, 13, 78513, 3376, 47559, 1473, 5159, 40853, 11, 304, 15506, 11, 3445, 1054, 34, 12806, 1154, 11453, 2100, 433, 574, 4762, 832, 315, 856, 38618, 889, 36592, 279, 38525, 8205, 382, 19, 13, 17958, 58716, 47559, 1473, 42493, 9332, 288, 374, 1457, 3967, 311, 617, 8208, 311, 41406, 3907, 11, 1405, 813, 836, 8111, 304, 279, 7576, 13, 1283, 574, 60044, 505, 6498, 23978, 1606, 813, 2536, 444, 630, 380, 3070, 1550, 539, 617, 1461, 43724, 4147, 304, 279, 9441, 315, 9635, 13, 320, 54567, 45480, 753, 2536, 444, 630, 380, 3070, 9960, 872, 43177, 555, 3515, 8945, 15274, 43724, 4147, 304, 279, 3623, 36, 11, 8051, 8530, 814, 11846, 433, 3287, 1431, 1797, 439, 264, 74927, 2533, 15274, 1047, 912, 2019, 304, 433, 13, 350, 380, 374, 3249, 568, 574, 3025, 311, 733, 311, 3771, 753, 9304, 11, 24562, 9456, 20, 13, 1054, 99280, 863, 374, 459, 2362, 6498, 3492, 369, 74873, 11, 902, 83417, 304, 1054, 56745, 1776, 675, 11453, 578, 6063, 3492, 374, 1054, 42, 40437, 11453, 320, 8538, 5269, 11, 358, 1541, 1431, 1518, 5975, 477, 4900, 356, 57589, 1694, 264, 26569, 13336, 13, 28584, 5235, 9290, 430, 10846, 6342, 24298, 374, 46305, 320, 295, 1631, 30450, 570, 5046, 3388, 11, 420, 10430, 1436, 5353, 11291, 311, 1772, 264, 12607, 315, 271, 66314, 1054, 34655, 863, 24298, 198, 99402, 1148, 813, 1972, 40853, 374, 3677, 1271, 24928, 1274, 311, 2343, 6666, 311, 43529, 994, 814, 6865, 4339, 11, 584, 4805, 3345, 449, 279, 25552, 315, 279, 6187, 1473, 2181, 753, 3062, 311, 2343, 3345, 6666, 304, 2978, 13, 1789, 1667, 358, 3463, 430, 198, 65, 7596, 30443, 660, 682, 12688, 382, 2345, 35, 31936, 432, 13, 10357, 30132, 271, 21, 13, 5046, 3388, 11, 420, 10430, 1436, 5353, 11291, 311, 1772, 264, 12607, 315, 198, 66314, 6342, 24298, 198, 3348, 72, 434, 1148, 813, 1972, 40853, 374, 3677, 22, 13, 5046, 3388, 11, 420, 10430, 1436, 5353, 11291, 311, 1772, 264, 12607, 315, 198, 66314, 6342, 24298, 198, 3348, 72, 434, 1148, 813, 1972, 40853, 374, 3677, 791, 1176, 86205, 574, 856, 14867, 26, 279, 5066, 1584, 9137, 82, 304, 279, 2132, 4879, 198, 8793, 291, 1578, 1618, 8, 5101, 311, 387, 264, 502, 1054, 13043, 15397, 23, 13, 78513, 3376, 47559, 1473, 791, 38021, 1054, 56745, 863, 649, 387, 1766, 304, 6498, 58614, 439, 264, 14281, 836, 369, 11012, 315, 279, 74873, 3070, 13, 1102, 374, 5552, 311, 279, 6063, 597, 40437, 323, 1156, 1751, 819, 597, 607, 477, 63577, 13, 4314, 527, 682, 14592, 505, 279, 64019, 3492, 1400, 285, 320, 269, 2211, 65130, 8, 7438, 264, 19646, 11, 902, 374, 1101, 279, 3789, 315, 279, 3492, 96970, 382, 791, 40853, 1054, 99280, 863, 323, 279, 11678, 1054, 34, 7298, 863, 527, 14470, 4279, 304, 9635, 323, 23782, 11, 719, 527, 539, 5552, 311, 279, 64019, 3492, 369, 74873, 13, 11995, 527, 48416, 332, 1924, 315, 279, 836, 1054, 86279, 19736, 15397, 24, 13, 28624, 358, 8621, 264, 2697, 6710, 922, 99234, 19463, 13, 3011, 832, 323, 279, 3885, 430, 8272, 433, 320, 6881, 323, 1618, 8, 19168, 311, 387, 29392, 5526, 779, 358, 4070, 1027, 9293, 311, 923, 264, 2478, 810, 8158, 12996, 605, 13, 1102, 2736, 706, 264, 5526, 836, 25, 800, 343, 1565, 753, 2383, 315, 384, 621, 1631, 88, 13]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Tokenize a sample text\n",
    "sample_text = dataset[\"train\"][0][\"text\"]\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"Token IDs:\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56151fab48f045238548cc806281e03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d95172db6b40a1878a7d8e8aed3184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on small sample...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.91 GB, other allocations: 1.05 GB, max allowed: 18.13 GB). Tried to allocate 1002.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training on small sample...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/transformers/trainer.py:2531\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2524\u001b[39m context = (\n\u001b[32m   2525\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2526\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2527\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2529\u001b[39m )\n\u001b[32m   2530\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2531\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2534\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2535\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2536\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2537\u001b[39m ):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2539\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/transformers/trainer.py:3712\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3710\u001b[39m     loss = loss / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n\u001b[32m-> \u001b[39m\u001b[32m3712\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3714\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/accelerate/accelerator.py:2248\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2247\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2248\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 16.91 GB, other allocations: 1.05 GB, max allowed: 18.13 GB). Tried to allocate 1002.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "small_train_dataset = train_dataset.select(range(100))\n",
    "small_eval_dataset = eval_dataset.select(range(50))\n",
    "\n",
    "# Assign a padding token to the tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use the eos_token as the pad_token\n",
    "\n",
    "# Tokenize the small sample dataset\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Add labels for loss computation\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-math-pretrain-small\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10, \n",
    "    save_steps=10, \n",
    "    per_device_train_batch_size=1, \n",
    "    num_train_epochs=1,  # Train for 1 epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if supported\n",
    "    report_to=\"none\",  # Disable reporting to external services\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training on small sample...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(32, 15, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Visualize attention for the first layer and first head\u001b[39;00m\n\u001b[32m     14\u001b[39m attention = attention_scores[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].detach().numpy()  \u001b[38;5;66;03m# First layer, first head\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mviridis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mAttention Scores\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/seaborn/matrix.py:446\u001b[39m, in \u001b[36mheatmap\u001b[39m\u001b[34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[32m    366\u001b[39m \n\u001b[32m    367\u001b[39m \u001b[33;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m plotter = \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[32m    451\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mlinewidths\u001b[39m\u001b[33m\"\u001b[39m] = linewidths\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/seaborn/matrix.py:110\u001b[39m, in \u001b[36m_HeatMapper.__init__\u001b[39m\u001b[34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    109\u001b[39m     plot_data = np.asarray(data)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[32m    113\u001b[39m mask = _matrix_mask(data, mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/pandas/core/frame.py:827\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    816\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    817\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    818\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m             copy=_copy,\n\u001b[32m    825\u001b[39m         )\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:314\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    308\u001b[39m     _copy = (\n\u001b[32m    309\u001b[39m         copy_on_sanitize\n\u001b[32m    310\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m astype_is_view(values.dtype, dtype))\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m     values = np.array(values, copy=_copy)\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     values = \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[32m    319\u001b[39m     values = _prep_ndarraylike(values, copy=copy_on_sanitize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:592\u001b[39m, in \u001b[36m_ensure_2d\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    590\u001b[39m     values = values.reshape((values.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m))\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m values.ndim != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mValueError\u001b[39m: Must pass 2-d input. shape=(32, 15, 15)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example input\n",
    "inputs = tokenizer(\"Solve for x: 2x + 3 = 7\", return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass with attention outputs\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Access attention scores\n",
    "attention_scores = outputs.attentions  # List of attention scores for each layer\n",
    "\n",
    "# Visualize attention for the first layer and first head\n",
    "attention = attention_scores[0][0].detach().numpy()  # First layer, first head\n",
    "sns.heatmap(attention, cmap=\"viridis\")\n",
    "plt.title(\"Attention Scores\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
