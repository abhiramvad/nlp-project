{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (4.48.3)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install torch transformers datasets matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f935a432cec4dcc8b1f8f7a5d209cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a680a10b9402454285fd723e4209ffb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['url', 'text', 'date', 'metadata'],\n",
       "        num_rows: 6315233\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"open-web-math/open-web-math\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample from the dataset:\n",
      "{'url': 'https://telescoper.wordpress.com/2010/11/23/bayes-and-hi-theorem/', 'text': 'Bayes and his\\xa0Theorem\\n\\nMy earlier post on Bayesian probability seems to have generated quite a lot of readers, so this lunchtime I thought I’d add a little bit of background. The previous discussion started from the result\\n\\n$P(B|AC) = K^{-1}P(B|C)P(A|BC) = K^{-1} P(AB|C)$\\n\\nwhere\\n\\n$K=P(A|C).$\\n\\nAlthough this is called Bayes’ theorem, the general form of it as stated here was actually first written down, not by Bayes but by Laplace. What Bayes’ did was derive the special case of this formula for “inverting” the binomial distribution. This distribution gives the probability of x successes in n independent “trials” each having the same probability of success, p; each “trial” has only two possible outcomes (“success” or “failure”). Trials like this are usually called Bernoulli trials, after Daniel Bernoulli. If we ask the question “what is the probability of exactly x successes from the possible n?”, the answer is given by the binomial distribution:\\n\\n$P_n(x|n,p)= C(n,x) p^x (1-p)^{n-x}$\\n\\nwhere\\n\\n$C(n,x)= n!/x!(n-x)!$\\n\\nis the number of distinct combinations of x objects that can be drawn from a pool of n.\\n\\nYou can probably see immediately how this arises. The probability of x consecutive successes is p multiplied by itself x times, or px. The probability of (n-x) successive failures is similarly (1-p)n-x. The last two terms basically therefore tell us the probability that we have exactly x successes (since there must be n-x failures). The combinatorial factor in front takes account of the fact that the ordering of successes and failures doesn’t matter.\\n\\nThe binomial distribution applies, for example, to repeated tosses of a coin, in which case p is taken to be 0.5 for a fair coin. A biased coin might have a different value of p, but as long as the tosses are independent the formula still applies. The binomial distribution also applies to problems involving drawing balls from urns: it works exactly if the balls are replaced in the urn after each draw, but it also applies approximately without replacement, as long as the number of draws is much smaller than the number of balls in the urn. I leave it as an exercise to calculate the expectation value of the binomial distribution, but the result is not surprising: E(X)=np. If you toss a fair coin ten times the expectation value for the number of heads is 10 times 0.5, which is five. No surprise there. After another bit of maths, the variance of the distribution can also be found. It is np(1-p).\\n\\nSo this gives us the probability of x given a fixed value of p. Bayes was interested in the inverse of this result, the probability of p given x. In other words, Bayes was interested in the answer to the question “If I perform n independent trials and get x successes, what is the probability distribution of p?”. This is a classic example of inverse reasoning. He got the correct answer, eventually, but by very convoluted reasoning. In my opinion it is quite difficult to justify the name Bayes’ theorem based on what he actually did, although Laplace did specifically acknowledge this contribution when he derived the general result later, which is no doubt why the theorem is always named in Bayes’ honour.\\n\\nThis is not the only example in science where the wrong person’s name is attached to a result or discovery. In fact, it is almost a law of Nature that any theorem that has a name has the wrong name. I propose that this observation should henceforth be known as Coles’ Law.\\n\\nSo who was the mysterious mathematician behind this result? Thomas Bayes was born in 1702, son of Joshua Bayes, who was a Fellow of the Royal Society (FRS) and one of the very first nonconformist ministers to be ordained in England. Thomas was himself ordained and for a while worked with his father in the Presbyterian Meeting House in Leather Lane, near Holborn in London. In 1720 he was a minister in Tunbridge Wells, in Kent. He retired from the church in 1752 and died in 1761. Thomas Bayes didn’t publish a single paper on mathematics in his own name during his lifetime but despite this was elected a Fellow of the Royal Society (FRS) in 1742. Presumably he had Friends of the Right Sort. He did however write a paper on fluxions in 1736, which was published anonymously. This was probably the grounds on which he was elected an FRS.\\n\\nThe paper containing the theorem that now bears his name was published posthumously in the Philosophical Transactions of the Royal Society of London in 1764.\\n\\nP.S. I understand that the authenticity of the picture is open to question. Whoever it actually is, he looks\\xa0 to me a bit like Laurence Olivier…\\n\\n11 Responses to “Bayes and his\\xa0Theorem”\\n\\n1. Bryn Jones Says:\\n\\nThe Royal Society is providing free access to electronic versions of its journals until the end of this month. Readers of this blog might like to look at Thomas Bayes’s two posthumous publications in the Philosophical Transactions.\\n\\nThe first is a short paper about series. The other is the paper about statistics communicated by Richard Price. (The statistics paper may be accessible on a long-term basis because it is one of the Royal Society’s Trailblazing papers the society provides access to as part of its 350th anniversary celebrations.)\\n\\nIncidentally, both Thomas Bayes and Richard Price were buried in the Bunhill Fields Cemetery in London and their tombs can be seen there today.\\n\\n2. Steve Warren Says:\\n\\nYou may be remembered in history as the discoverer of coleslaw, but you weren’t the first.\\n\\n• Anton Garrett Says:\\n\\nFor years I thought it was “cold slaw” because it was served cold. A good job I never asked for warm slaw.\\n\\n3. telescoper Says:\\n\\nMy surname, in Spanish, means “Cabbages”. So it was probably one of my ancestors who invented the chopped variety.\\n\\n4. Anton Garrett Says:\\n\\nThomas Bayes is now known to have gone to Edinburgh University, where his name appears in the records. He was barred from English universities because his nonconformist family did not have him baptised in the Church of England. (Charles Darwin’s nonconformist family covered their bets by having baby Charles baptised in the CoE, although perhaps they believed it didn’t count as a baptism since Charles had no say in it. Tist is why he was able to go to Christ’s College, Cambridge.)\\n\\n5. “Cole” is an old English word for cabbage, which survives in “cole slaw”. The German word is “Kohl”. (Somehow, I don’t see PM or President Cabbage being a realistic possibility. 🙂 )\\n\\nNote that Old King Cole is unrelated (etymologically). Of course, this discussion could cause Peter to post a clip of\\n\\nNat “King” Cole\\n(guess what his real surname is).\\n\\nTo remind people to pay attention to spelling when they hear words, we’ll close with the Quote of the Day:\\n\\nIt’s important to pay close attention in school. For years I thought that\\nbears masturbated all winter.\\n\\n—Damon R. Milhem\\n\\n6. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\n7. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\nThe first typo was my fault; the extra linebreaks in the second attempt\\n(tested again here) appear to be a new “feature”.\\n\\n8. telescoper Says:\\n\\nThe noun “cole” can be found in English dictionaries as a generic name for plants of the cabbage family. It is related to the German kohl and scottish kail or kale. These are all derived from the latin word colis (or caulis) meaning a stem, which is also the root of the word cauliflower.\\n\\nThe surname “Cole” and the variant “Coles” are fairly common in England and Wales, but are not related to the latin word for cabbage. Both are diminutives of the name “Nicholas”.\\n\\n9. […] I posted a little piece about Bayesian probability. That one and the others that followed it (here and here) proved to be surprisingly popular so I’ve been planning to add a few more posts […]\\n\\n10. It already has a popular name: Stigler’s law of eponymy.', 'date': '2016-09-28 22:12:05', 'metadata': '{\"extraction_info\": {\"found_math\": true, \"script_math_tex\": 0, \"script_math_asciimath\": 0, \"math_annotations\": 0, \"math_alttext\": 0, \"mathml\": 0, \"mathjax_tag\": 0, \"mathjax_inline_tex\": 0, \"mathjax_display_tex\": 0, \"mathjax_asciimath\": 0, \"img_math\": 4, \"codecogs_latex\": 0, \"wp_latex\": 0, \"mimetex.cgi\": 0, \"/images/math/codecogs\": 0, \"mathtex.cgi\": 0, \"katex\": 0, \"math-container\": 0, \"wp-katex-eq\": 0, \"align\": 0, \"equation\": 0, \"x-ck12\": 0, \"texerror\": 0, \"math_score\": 0.6193313002586365, \"perplexity\": 1089.0032368849284}, \"config\": {\"markdown_headings\": false, \"markdown_code\": true, \"boilerplate_config\": {\"ratio_threshold\": 0.18, \"absolute_threshold\": 10, \"end_threshold\": 15, \"enable\": true}, \"remove_buttons\": true, \"remove_image_figures\": true, \"remove_link_clusters\": true, \"table_config\": {\"min_rows\": 2, \"min_cols\": 3, \"format\": \"plain\"}, \"remove_chinese\": true, \"remove_edit_buttons\": true, \"extract_latex\": true}, \"warc_path\": \"s3://commoncrawl/crawl-data/CC-MAIN-2016-40/segments/1474738661768.10/warc/CC-MAIN-20160924173741-00020-ip-10-143-35-109.ec2.internal.warc.gz\"}'}\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['url', 'text', 'date', 'metadata'],\n",
      "        num_rows: 6315233\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Display a sample from the dataset\n",
    "print(\"Sample from the dataset:\")\n",
    "print(dataset[\"train\"][0])\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bayes and his\\xa0Theorem\\n\\nMy earlier post on Bayesian probability seems to have generated quite a lot of readers, so this lunchtime I thought I’d add a little bit of background. The previous discussion started from the result\\n\\n$P(B|AC) = K^{-1}P(B|C)P(A|BC) = K^{-1} P(AB|C)$\\n\\nwhere\\n\\n$K=P(A|C).$\\n\\nAlthough this is called Bayes’ theorem, the general form of it as stated here was actually first written down, not by Bayes but by Laplace. What Bayes’ did was derive the special case of this formula for “inverting” the binomial distribution. This distribution gives the probability of x successes in n independent “trials” each having the same probability of success, p; each “trial” has only two possible outcomes (“success” or “failure”). Trials like this are usually called Bernoulli trials, after Daniel Bernoulli. If we ask the question “what is the probability of exactly x successes from the possible n?”, the answer is given by the binomial distribution:\\n\\n$P_n(x|n,p)= C(n,x) p^x (1-p)^{n-x}$\\n\\nwhere\\n\\n$C(n,x)= n!/x!(n-x)!$\\n\\nis the number of distinct combinations of x objects that can be drawn from a pool of n.\\n\\nYou can probably see immediately how this arises. The probability of x consecutive successes is p multiplied by itself x times, or px. The probability of (n-x) successive failures is similarly (1-p)n-x. The last two terms basically therefore tell us the probability that we have exactly x successes (since there must be n-x failures). The combinatorial factor in front takes account of the fact that the ordering of successes and failures doesn’t matter.\\n\\nThe binomial distribution applies, for example, to repeated tosses of a coin, in which case p is taken to be 0.5 for a fair coin. A biased coin might have a different value of p, but as long as the tosses are independent the formula still applies. The binomial distribution also applies to problems involving drawing balls from urns: it works exactly if the balls are replaced in the urn after each draw, but it also applies approximately without replacement, as long as the number of draws is much smaller than the number of balls in the urn. I leave it as an exercise to calculate the expectation value of the binomial distribution, but the result is not surprising: E(X)=np. If you toss a fair coin ten times the expectation value for the number of heads is 10 times 0.5, which is five. No surprise there. After another bit of maths, the variance of the distribution can also be found. It is np(1-p).\\n\\nSo this gives us the probability of x given a fixed value of p. Bayes was interested in the inverse of this result, the probability of p given x. In other words, Bayes was interested in the answer to the question “If I perform n independent trials and get x successes, what is the probability distribution of p?”. This is a classic example of inverse reasoning. He got the correct answer, eventually, but by very convoluted reasoning. In my opinion it is quite difficult to justify the name Bayes’ theorem based on what he actually did, although Laplace did specifically acknowledge this contribution when he derived the general result later, which is no doubt why the theorem is always named in Bayes’ honour.\\n\\nThis is not the only example in science where the wrong person’s name is attached to a result or discovery. In fact, it is almost a law of Nature that any theorem that has a name has the wrong name. I propose that this observation should henceforth be known as Coles’ Law.\\n\\nSo who was the mysterious mathematician behind this result? Thomas Bayes was born in 1702, son of Joshua Bayes, who was a Fellow of the Royal Society (FRS) and one of the very first nonconformist ministers to be ordained in England. Thomas was himself ordained and for a while worked with his father in the Presbyterian Meeting House in Leather Lane, near Holborn in London. In 1720 he was a minister in Tunbridge Wells, in Kent. He retired from the church in 1752 and died in 1761. Thomas Bayes didn’t publish a single paper on mathematics in his own name during his lifetime but despite this was elected a Fellow of the Royal Society (FRS) in 1742. Presumably he had Friends of the Right Sort. He did however write a paper on fluxions in 1736, which was published anonymously. This was probably the grounds on which he was elected an FRS.\\n\\nThe paper containing the theorem that now bears his name was published posthumously in the Philosophical Transactions of the Royal Society of London in 1764.\\n\\nP.S. I understand that the authenticity of the picture is open to question. Whoever it actually is, he looks\\xa0 to me a bit like Laurence Olivier…\\n\\n11 Responses to “Bayes and his\\xa0Theorem”\\n\\n1. Bryn Jones Says:\\n\\nThe Royal Society is providing free access to electronic versions of its journals until the end of this month. Readers of this blog might like to look at Thomas Bayes’s two posthumous publications in the Philosophical Transactions.\\n\\nThe first is a short paper about series. The other is the paper about statistics communicated by Richard Price. (The statistics paper may be accessible on a long-term basis because it is one of the Royal Society’s Trailblazing papers the society provides access to as part of its 350th anniversary celebrations.)\\n\\nIncidentally, both Thomas Bayes and Richard Price were buried in the Bunhill Fields Cemetery in London and their tombs can be seen there today.\\n\\n2. Steve Warren Says:\\n\\nYou may be remembered in history as the discoverer of coleslaw, but you weren’t the first.\\n\\n• Anton Garrett Says:\\n\\nFor years I thought it was “cold slaw” because it was served cold. A good job I never asked for warm slaw.\\n\\n3. telescoper Says:\\n\\nMy surname, in Spanish, means “Cabbages”. So it was probably one of my ancestors who invented the chopped variety.\\n\\n4. Anton Garrett Says:\\n\\nThomas Bayes is now known to have gone to Edinburgh University, where his name appears in the records. He was barred from English universities because his nonconformist family did not have him baptised in the Church of England. (Charles Darwin’s nonconformist family covered their bets by having baby Charles baptised in the CoE, although perhaps they believed it didn’t count as a baptism since Charles had no say in it. Tist is why he was able to go to Christ’s College, Cambridge.)\\n\\n5. “Cole” is an old English word for cabbage, which survives in “cole slaw”. The German word is “Kohl”. (Somehow, I don’t see PM or President Cabbage being a realistic possibility. 🙂 )\\n\\nNote that Old King Cole is unrelated (etymologically). Of course, this discussion could cause Peter to post a clip of\\n\\nNat “King” Cole\\n(guess what his real surname is).\\n\\nTo remind people to pay attention to spelling when they hear words, we’ll close with the Quote of the Day:\\n\\nIt’s important to pay close attention in school. For years I thought that\\nbears masturbated all winter.\\n\\n—Damon R. Milhem\\n\\n6. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\n7. Of course, this discussion could cause Peter to post a clip of\\nNat King Cole\\n(giess what his real surname is).\\n\\nThe first typo was my fault; the extra linebreaks in the second attempt\\n(tested again here) appear to be a new “feature”.\\n\\n8. telescoper Says:\\n\\nThe noun “cole” can be found in English dictionaries as a generic name for plants of the cabbage family. It is related to the German kohl and scottish kail or kale. These are all derived from the latin word colis (or caulis) meaning a stem, which is also the root of the word cauliflower.\\n\\nThe surname “Cole” and the variant “Coles” are fairly common in England and Wales, but are not related to the latin word for cabbage. Both are diminutives of the name “Nicholas”.\\n\\n9. […] I posted a little piece about Bayesian probability. That one and the others that followed it (here and here) proved to be surprisingly popular so I’ve been planning to add a few more posts […]\\n\\n10. It already has a popular name: Stigler’s law of eponymy.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Tokenizer Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Sample text:\n",
      "Bayes and his Theorem\n",
      "\n",
      "My earlier post on Bayesian probability seems to have generated quite a lot of readers, so this lunchtime I thought I’d add a little bit of background. The previous discussion started from the result\n",
      "\n",
      "$P(B|AC) = K^{-1}P(B|C)P(A|BC) = K^{-1} P(AB|C)$\n",
      "\n",
      "where\n",
      "\n",
      "$K=P(A|C).$\n",
      "\n",
      "Although this is called Bayes’ theorem, the general form of it as stated here was actually first written down, not by Bayes but by Laplace. What Bayes’ did was derive the special case of this formula for “inverting” the binomial distribution. This distribution gives the probability of x successes in n independent “trials” each having the same probability of success, p; each “trial” has only two possible outcomes (“success” or “failure”). Trials like this are usually called Bernoulli trials, after Daniel Bernoulli. If we ask the question “what is the probability of exactly x successes from the possible n?”, the answer is given by the binomial distribution:\n",
      "\n",
      "$P_n(x|n,p)= C(n,x) p^x (1-p)^{n-x}$\n",
      "\n",
      "where\n",
      "\n",
      "$C(n,x)= n!/x!(n-x)!$\n",
      "\n",
      "is the number of distinct combinations of x objects that can be drawn from a pool of n.\n",
      "\n",
      "You can probably see immediately how this arises. The probability of x consecutive successes is p multiplied by itself x times, or px. The probability of (n-x) successive failures is similarly (1-p)n-x. The last two terms basically therefore tell us the probability that we have exactly x successes (since there must be n-x failures). The combinatorial factor in front takes account of the fact that the ordering of successes and failures doesn’t matter.\n",
      "\n",
      "The binomial distribution applies, for example, to repeated tosses of a coin, in which case p is taken to be 0.5 for a fair coin. A biased coin might have a different value of p, but as long as the tosses are independent the formula still applies. The binomial distribution also applies to problems involving drawing balls from urns: it works exactly if the balls are replaced in the urn after each draw, but it also applies approximately without replacement, as long as the number of draws is much smaller than the number of balls in the urn. I leave it as an exercise to calculate the expectation value of the binomial distribution, but the result is not surprising: E(X)=np. If you toss a fair coin ten times the expectation value for the number of heads is 10 times 0.5, which is five. No surprise there. After another bit of maths, the variance of the distribution can also be found. It is np(1-p).\n",
      "\n",
      "So this gives us the probability of x given a fixed value of p. Bayes was interested in the inverse of this result, the probability of p given x. In other words, Bayes was interested in the answer to the question “If I perform n independent trials and get x successes, what is the probability distribution of p?”. This is a classic example of inverse reasoning. He got the correct answer, eventually, but by very convoluted reasoning. In my opinion it is quite difficult to justify the name Bayes’ theorem based on what he actually did, although Laplace did specifically acknowledge this contribution when he derived the general result later, which is no doubt why the theorem is always named in Bayes’ honour.\n",
      "\n",
      "This is not the only example in science where the wrong person’s name is attached to a result or discovery. In fact, it is almost a law of Nature that any theorem that has a name has the wrong name. I propose that this observation should henceforth be known as Coles’ Law.\n",
      "\n",
      "So who was the mysterious mathematician behind this result? Thomas Bayes was born in 1702, son of Joshua Bayes, who was a Fellow of the Royal Society (FRS) and one of the very first nonconformist ministers to be ordained in England. Thomas was himself ordained and for a while worked with his father in the Presbyterian Meeting House in Leather Lane, near Holborn in London. In 1720 he was a minister in Tunbridge Wells, in Kent. He retired from the church in 1752 and died in 1761. Thomas Bayes didn’t publish a single paper on mathematics in his own name during his lifetime but despite this was elected a Fellow of the Royal Society (FRS) in 1742. Presumably he had Friends of the Right Sort. He did however write a paper on fluxions in 1736, which was published anonymously. This was probably the grounds on which he was elected an FRS.\n",
      "\n",
      "The paper containing the theorem that now bears his name was published posthumously in the Philosophical Transactions of the Royal Society of London in 1764.\n",
      "\n",
      "P.S. I understand that the authenticity of the picture is open to question. Whoever it actually is, he looks  to me a bit like Laurence Olivier…\n",
      "\n",
      "11 Responses to “Bayes and his Theorem”\n",
      "\n",
      "1. Bryn Jones Says:\n",
      "\n",
      "The Royal Society is providing free access to electronic versions of its journals until the end of this month. Readers of this blog might like to look at Thomas Bayes’s two posthumous publications in the Philosophical Transactions.\n",
      "\n",
      "The first is a short paper about series. The other is the paper about statistics communicated by Richard Price. (The statistics paper may be accessible on a long-term basis because it is one of the Royal Society’s Trailblazing papers the society provides access to as part of its 350th anniversary celebrations.)\n",
      "\n",
      "Incidentally, both Thomas Bayes and Richard Price were buried in the Bunhill Fields Cemetery in London and their tombs can be seen there today.\n",
      "\n",
      "2. Steve Warren Says:\n",
      "\n",
      "You may be remembered in history as the discoverer of coleslaw, but you weren’t the first.\n",
      "\n",
      "• Anton Garrett Says:\n",
      "\n",
      "For years I thought it was “cold slaw” because it was served cold. A good job I never asked for warm slaw.\n",
      "\n",
      "3. telescoper Says:\n",
      "\n",
      "My surname, in Spanish, means “Cabbages”. So it was probably one of my ancestors who invented the chopped variety.\n",
      "\n",
      "4. Anton Garrett Says:\n",
      "\n",
      "Thomas Bayes is now known to have gone to Edinburgh University, where his name appears in the records. He was barred from English universities because his nonconformist family did not have him baptised in the Church of England. (Charles Darwin’s nonconformist family covered their bets by having baby Charles baptised in the CoE, although perhaps they believed it didn’t count as a baptism since Charles had no say in it. Tist is why he was able to go to Christ’s College, Cambridge.)\n",
      "\n",
      "5. “Cole” is an old English word for cabbage, which survives in “cole slaw”. The German word is “Kohl”. (Somehow, I don’t see PM or President Cabbage being a realistic possibility. 🙂 )\n",
      "\n",
      "Note that Old King Cole is unrelated (etymologically). Of course, this discussion could cause Peter to post a clip of\n",
      "\n",
      "Nat “King” Cole\n",
      "(guess what his real surname is).\n",
      "\n",
      "To remind people to pay attention to spelling when they hear words, we’ll close with the Quote of the Day:\n",
      "\n",
      "It’s important to pay close attention in school. For years I thought that\n",
      "bears masturbated all winter.\n",
      "\n",
      "—Damon R. Milhem\n",
      "\n",
      "6. Of course, this discussion could cause Peter to post a clip of\n",
      "Nat King Cole\n",
      "(giess what his real surname is).\n",
      "\n",
      "7. Of course, this discussion could cause Peter to post a clip of\n",
      "Nat King Cole\n",
      "(giess what his real surname is).\n",
      "\n",
      "The first typo was my fault; the extra linebreaks in the second attempt\n",
      "(tested again here) appear to be a new “feature”.\n",
      "\n",
      "8. telescoper Says:\n",
      "\n",
      "The noun “cole” can be found in English dictionaries as a generic name for plants of the cabbage family. It is related to the German kohl and scottish kail or kale. These are all derived from the latin word colis (or caulis) meaning a stem, which is also the root of the word cauliflower.\n",
      "\n",
      "The surname “Cole” and the variant “Coles” are fairly common in England and Wales, but are not related to the latin word for cabbage. Both are diminutives of the name “Nicholas”.\n",
      "\n",
      "9. […] I posted a little piece about Bayesian probability. That one and the others that followed it (here and here) proved to be surprisingly popular so I’ve been planning to add a few more posts […]\n",
      "\n",
      "10. It already has a popular name: Stigler’s law of eponymy.\n",
      "Tokens:\n",
      "['Bay', 'es', 'Ġand', 'Ġhis', 'Âł', 'The', 'orem', 'ĊĊ', 'My', 'Ġearlier', 'Ġpost', 'Ġon', 'ĠBayesian', 'Ġprobability', 'Ġseems', 'Ġto', 'Ġhave', 'Ġgenerated', 'Ġquite', 'Ġa', 'Ġlot', 'Ġof', 'Ġreaders', ',', 'Ġso', 'Ġthis', 'Ġlunch', 'time', 'ĠI', 'Ġthought', 'ĠI', 'âĢĻd', 'Ġadd', 'Ġa', 'Ġlittle', 'Ġbit', 'Ġof', 'Ġbackground', '.', 'ĠThe', 'Ġprevious', 'Ġdiscussion', 'Ġstarted', 'Ġfrom', 'Ġthe', 'Ġresult', 'ĊĊ', '$', 'P', '(B', '|', 'AC', ')', 'Ġ=', 'ĠK', '^{-', '1', '}', 'P', '(B', '|', 'C', ')', 'P', '(A', '|', 'BC', ')', 'Ġ=', 'ĠK', '^{-', '1', '}', 'ĠP', '(', 'AB', '|', 'C', ')$', 'ĊĊ', 'where', 'ĊĊ', '$', 'K', '=P', '(A', '|', 'C', ').', '$ĊĊ', 'Although', 'Ġthis', 'Ġis', 'Ġcalled', 'ĠBay', 'es', 'âĢĻ', 'Ġtheorem', ',', 'Ġthe', 'Ġgeneral', 'Ġform', 'Ġof', 'Ġit', 'Ġas', 'Ġstated', 'Ġhere', 'Ġwas', 'Ġactually', 'Ġfirst', 'Ġwritten', 'Ġdown', ',', 'Ġnot', 'Ġby', 'ĠBay', 'es', 'Ġbut', 'Ġby', 'ĠLap', 'lace', '.', 'ĠWhat', 'ĠBay', 'es', 'âĢĻ', 'Ġdid', 'Ġwas', 'Ġderive', 'Ġthe', 'Ġspecial', 'Ġcase', 'Ġof', 'Ġthis', 'Ġformula', 'Ġfor', 'ĠâĢľ', 'in', 'verting', 'âĢĿ', 'Ġthe', 'Ġbin', 'omial', 'Ġdistribution', '.', 'ĠThis', 'Ġdistribution', 'Ġgives', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġx', 'Ġsuccesses', 'Ġin', 'Ġn', 'Ġindependent', 'ĠâĢľ', 'tr', 'ials', 'âĢĿ', 'Ġeach', 'Ġhaving', 'Ġthe', 'Ġsame', 'Ġprobability', 'Ġof', 'Ġsuccess', ',', 'Ġp', ';', 'Ġeach', 'ĠâĢľ', 'trial', 'âĢĿ', 'Ġhas', 'Ġonly', 'Ġtwo', 'Ġpossible', 'Ġoutcomes', 'Ġ(âĢľ', 'success', 'âĢĿ', 'Ġor', 'ĠâĢľ', 'failure', 'âĢĿ).', 'ĠTrials', 'Ġlike', 'Ġthis', 'Ġare', 'Ġusually', 'Ġcalled', 'ĠBern', 'ou', 'lli', 'Ġtrials', ',', 'Ġafter', 'ĠDaniel', 'ĠBern', 'ou', 'lli', '.', 'ĠIf', 'Ġwe', 'Ġask', 'Ġthe', 'Ġquestion', 'ĠâĢľ', 'what', 'Ġis', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġexactly', 'Ġx', 'Ġsuccesses', 'Ġfrom', 'Ġthe', 'Ġpossible', 'Ġn', '?', 'âĢĿ,', 'Ġthe', 'Ġanswer', 'Ġis', 'Ġgiven', 'Ġby', 'Ġthe', 'Ġbin', 'omial', 'Ġdistribution', ':ĊĊ', '$', 'P', '_n', '(x', '|', 'n', ',p', ')=', 'ĠC', '(n', ',x', ')', 'Ġp', '^', 'x', 'Ġ(', '1', '-p', ')^', '{', 'n', '-x', '}$', 'ĊĊ', 'where', 'ĊĊ', '$', 'C', '(n', ',x', ')=', 'Ġn', '!/', 'x', '!(', 'n', '-x', ')!', '$ĊĊ', 'is', 'Ġthe', 'Ġnumber', 'Ġof', 'Ġdistinct', 'Ġcombinations', 'Ġof', 'Ġx', 'Ġobjects', 'Ġthat', 'Ġcan', 'Ġbe', 'Ġdrawn', 'Ġfrom', 'Ġa', 'Ġpool', 'Ġof', 'Ġn', '.ĊĊ', 'You', 'Ġcan', 'Ġprobably', 'Ġsee', 'Ġimmediately', 'Ġhow', 'Ġthis', 'Ġarises', '.', 'ĠThe', 'Ġprobability', 'Ġof', 'Ġx', 'Ġconsecutive', 'Ġsuccesses', 'Ġis', 'Ġp', 'Ġmultiplied', 'Ġby', 'Ġitself', 'Ġx', 'Ġtimes', ',', 'Ġor', 'Ġpx', '.', 'ĠThe', 'Ġprobability', 'Ġof', 'Ġ(', 'n', '-x', ')', 'Ġsuccessive', 'Ġfailures', 'Ġis', 'Ġsimilarly', 'Ġ(', '1', '-p', ')n', '-x', '.', 'ĠThe', 'Ġlast', 'Ġtwo', 'Ġterms', 'Ġbasically', 'Ġtherefore', 'Ġtell', 'Ġus', 'Ġthe', 'Ġprobability', 'Ġthat', 'Ġwe', 'Ġhave', 'Ġexactly', 'Ġx', 'Ġsuccesses', 'Ġ(', 'since', 'Ġthere', 'Ġmust', 'Ġbe', 'Ġn', '-x', 'Ġfailures', ').', 'ĠThe', 'Ġcomb', 'inator', 'ial', 'Ġfactor', 'Ġin', 'Ġfront', 'Ġtakes', 'Ġaccount', 'Ġof', 'Ġthe', 'Ġfact', 'Ġthat', 'Ġthe', 'Ġordering', 'Ġof', 'Ġsuccesses', 'Ġand', 'Ġfailures', 'Ġdoesn', 'âĢĻt', 'Ġmatter', '.ĊĊ', 'The', 'Ġbin', 'omial', 'Ġdistribution', 'Ġapplies', ',', 'Ġfor', 'Ġexample', ',', 'Ġto', 'Ġrepeated', 'Ġtoss', 'es', 'Ġof', 'Ġa', 'Ġcoin', ',', 'Ġin', 'Ġwhich', 'Ġcase', 'Ġp', 'Ġis', 'Ġtaken', 'Ġto', 'Ġbe', 'Ġ', '0', '.', '5', 'Ġfor', 'Ġa', 'Ġfair', 'Ġcoin', '.', 'ĠA', 'Ġbiased', 'Ġcoin', 'Ġmight', 'Ġhave', 'Ġa', 'Ġdifferent', 'Ġvalue', 'Ġof', 'Ġp', ',', 'Ġbut', 'Ġas', 'Ġlong', 'Ġas', 'Ġthe', 'Ġtoss', 'es', 'Ġare', 'Ġindependent', 'Ġthe', 'Ġformula', 'Ġstill', 'Ġapplies', '.', 'ĠThe', 'Ġbin', 'omial', 'Ġdistribution', 'Ġalso', 'Ġapplies', 'Ġto', 'Ġproblems', 'Ġinvolving', 'Ġdrawing', 'Ġballs', 'Ġfrom', 'Ġurn', 's', ':', 'Ġit', 'Ġworks', 'Ġexactly', 'Ġif', 'Ġthe', 'Ġballs', 'Ġare', 'Ġreplaced', 'Ġin', 'Ġthe', 'Ġurn', 'Ġafter', 'Ġeach', 'Ġdraw', ',', 'Ġbut', 'Ġit', 'Ġalso', 'Ġapplies', 'Ġapproximately', 'Ġwithout', 'Ġreplacement', ',', 'Ġas', 'Ġlong', 'Ġas', 'Ġthe', 'Ġnumber', 'Ġof', 'Ġdraws', 'Ġis', 'Ġmuch', 'Ġsmaller', 'Ġthan', 'Ġthe', 'Ġnumber', 'Ġof', 'Ġballs', 'Ġin', 'Ġthe', 'Ġurn', '.', 'ĠI', 'Ġleave', 'Ġit', 'Ġas', 'Ġan', 'Ġexercise', 'Ġto', 'Ġcalculate', 'Ġthe', 'Ġexpectation', 'Ġvalue', 'Ġof', 'Ġthe', 'Ġbin', 'omial', 'Ġdistribution', ',', 'Ġbut', 'Ġthe', 'Ġresult', 'Ġis', 'Ġnot', 'Ġsurprising', ':', 'ĠE', '(X', ')=', 'np', '.', 'ĠIf', 'Ġyou', 'Ġtoss', 'Ġa', 'Ġfair', 'Ġcoin', 'Ġten', 'Ġtimes', 'Ġthe', 'Ġexpectation', 'Ġvalue', 'Ġfor', 'Ġthe', 'Ġnumber', 'Ġof', 'Ġheads', 'Ġis', 'Ġ', '10', 'Ġtimes', 'Ġ', '0', '.', '5', ',', 'Ġwhich', 'Ġis', 'Ġfive', '.', 'ĠNo', 'Ġsurprise', 'Ġthere', '.', 'ĠAfter', 'Ġanother', 'Ġbit', 'Ġof', 'Ġmaths', ',', 'Ġthe', 'Ġvariance', 'Ġof', 'Ġthe', 'Ġdistribution', 'Ġcan', 'Ġalso', 'Ġbe', 'Ġfound', '.', 'ĠIt', 'Ġis', 'Ġnp', '(', '1', '-p', ').ĊĊ', 'So', 'Ġthis', 'Ġgives', 'Ġus', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġx', 'Ġgiven', 'Ġa', 'Ġfixed', 'Ġvalue', 'Ġof', 'Ġp', '.', 'ĠBay', 'es', 'Ġwas', 'Ġinterested', 'Ġin', 'Ġthe', 'Ġinverse', 'Ġof', 'Ġthis', 'Ġresult', ',', 'Ġthe', 'Ġprobability', 'Ġof', 'Ġp', 'Ġgiven', 'Ġx', '.', 'ĠIn', 'Ġother', 'Ġwords', ',', 'ĠBay', 'es', 'Ġwas', 'Ġinterested', 'Ġin', 'Ġthe', 'Ġanswer', 'Ġto', 'Ġthe', 'Ġquestion', 'ĠâĢľ', 'If', 'ĠI', 'Ġperform', 'Ġn', 'Ġindependent', 'Ġtrials', 'Ġand', 'Ġget', 'Ġx', 'Ġsuccesses', ',', 'Ġwhat', 'Ġis', 'Ġthe', 'Ġprobability', 'Ġdistribution', 'Ġof', 'Ġp', '?', 'âĢĿ.', 'ĠThis', 'Ġis', 'Ġa', 'Ġclassic', 'Ġexample', 'Ġof', 'Ġinverse', 'Ġreasoning', '.', 'ĠHe', 'Ġgot', 'Ġthe', 'Ġcorrect', 'Ġanswer', ',', 'Ġeventually', ',', 'Ġbut', 'Ġby', 'Ġvery', 'Ġconv', 'ol', 'uted', 'Ġreasoning', '.', 'ĠIn', 'Ġmy', 'Ġopinion', 'Ġit', 'Ġis', 'Ġquite', 'Ġdifficult', 'Ġto', 'Ġjustify', 'Ġthe', 'Ġname', 'ĠBay', 'es', 'âĢĻ', 'Ġtheorem', 'Ġbased', 'Ġon', 'Ġwhat', 'Ġhe', 'Ġactually', 'Ġdid', ',', 'Ġalthough', 'ĠLap', 'lace', 'Ġdid', 'Ġspecifically', 'Ġacknowledge', 'Ġthis', 'Ġcontribution', 'Ġwhen', 'Ġhe', 'Ġderived', 'Ġthe', 'Ġgeneral', 'Ġresult', 'Ġlater', ',', 'Ġwhich', 'Ġis', 'Ġno', 'Ġdoubt', 'Ġwhy', 'Ġthe', 'Ġtheorem', 'Ġis', 'Ġalways', 'Ġnamed', 'Ġin', 'ĠBay', 'es', 'âĢĻ', 'Ġhonour', '.ĊĊ', 'This', 'Ġis', 'Ġnot', 'Ġthe', 'Ġonly', 'Ġexample', 'Ġin', 'Ġscience', 'Ġwhere', 'Ġthe', 'Ġwrong', 'Ġperson', 'âĢĻs', 'Ġname', 'Ġis', 'Ġattached', 'Ġto', 'Ġa', 'Ġresult', 'Ġor', 'Ġdiscovery', '.', 'ĠIn', 'Ġfact', ',', 'Ġit', 'Ġis', 'Ġalmost', 'Ġa', 'Ġlaw', 'Ġof', 'ĠNature', 'Ġthat', 'Ġany', 'Ġtheorem', 'Ġthat', 'Ġhas', 'Ġa', 'Ġname', 'Ġhas', 'Ġthe', 'Ġwrong', 'Ġname', '.', 'ĠI', 'Ġpropose', 'Ġthat', 'Ġthis', 'Ġobservation', 'Ġshould', 'Ġhence', 'forth', 'Ġbe', 'Ġknown', 'Ġas', 'ĠCo', 'les', 'âĢĻ', 'ĠLaw', '.ĊĊ', 'So', 'Ġwho', 'Ġwas', 'Ġthe', 'Ġmysterious', 'Ġmathematic', 'ian', 'Ġbehind', 'Ġthis', 'Ġresult', '?', 'ĠThomas', 'ĠBay', 'es', 'Ġwas', 'Ġborn', 'Ġin', 'Ġ', '170', '2', ',', 'Ġson', 'Ġof', 'ĠJoshua', 'ĠBay', 'es', ',', 'Ġwho', 'Ġwas', 'Ġa', 'ĠFellow', 'Ġof', 'Ġthe', 'ĠRoyal', 'ĠSociety', 'Ġ(', 'FR', 'S', ')', 'Ġand', 'Ġone', 'Ġof', 'Ġthe', 'Ġvery', 'Ġfirst', 'Ġnon', 'con', 'form', 'ist', 'Ġministers', 'Ġto', 'Ġbe', 'Ġordained', 'Ġin', 'ĠEngland', '.', 'ĠThomas', 'Ġwas', 'Ġhimself', 'Ġordained', 'Ġand', 'Ġfor', 'Ġa', 'Ġwhile', 'Ġworked', 'Ġwith', 'Ġhis', 'Ġfather', 'Ġin', 'Ġthe', 'ĠPresbyterian', 'ĠMeeting', 'ĠHouse', 'Ġin', 'ĠLeather', 'ĠLane', ',', 'Ġnear', 'ĠHol', 'born', 'Ġin', 'ĠLondon', '.', 'ĠIn', 'Ġ', '172', '0', 'Ġhe', 'Ġwas', 'Ġa', 'Ġminister', 'Ġin', 'ĠTun', 'bridge', 'ĠWells', ',', 'Ġin', 'ĠKent', '.', 'ĠHe', 'Ġretired', 'Ġfrom', 'Ġthe', 'Ġchurch', 'Ġin', 'Ġ', '175', '2', 'Ġand', 'Ġdied', 'Ġin', 'Ġ', '176', '1', '.', 'ĠThomas', 'ĠBay', 'es', 'Ġdidn', 'âĢĻt', 'Ġpublish', 'Ġa', 'Ġsingle', 'Ġpaper', 'Ġon', 'Ġmathematics', 'Ġin', 'Ġhis', 'Ġown', 'Ġname', 'Ġduring', 'Ġhis', 'Ġlifetime', 'Ġbut', 'Ġdespite', 'Ġthis', 'Ġwas', 'Ġelected', 'Ġa', 'ĠFellow', 'Ġof', 'Ġthe', 'ĠRoyal', 'ĠSociety', 'Ġ(', 'FR', 'S', ')', 'Ġin', 'Ġ', '174', '2', '.', 'ĠPres', 'umably', 'Ġhe', 'Ġhad', 'ĠFriends', 'Ġof', 'Ġthe', 'ĠRight', 'ĠSort', '.', 'ĠHe', 'Ġdid', 'Ġhowever', 'Ġwrite', 'Ġa', 'Ġpaper', 'Ġon', 'Ġflux', 'ions', 'Ġin', 'Ġ', '173', '6', ',', 'Ġwhich', 'Ġwas', 'Ġpublished', 'Ġanonymously', '.', 'ĠThis', 'Ġwas', 'Ġprobably', 'Ġthe', 'Ġgrounds', 'Ġon', 'Ġwhich', 'Ġhe', 'Ġwas', 'Ġelected', 'Ġan', 'ĠF', 'RS', '.ĊĊ', 'The', 'Ġpaper', 'Ġcontaining', 'Ġthe', 'Ġtheorem', 'Ġthat', 'Ġnow', 'Ġbears', 'Ġhis', 'Ġname', 'Ġwas', 'Ġpublished', 'Ġpost', 'hum', 'ously', 'Ġin', 'Ġthe', 'ĠPhilosoph', 'ical', 'ĠTransactions', 'Ġof', 'Ġthe', 'ĠRoyal', 'ĠSociety', 'Ġof', 'ĠLondon', 'Ġin', 'Ġ', '176', '4', '.ĊĊ', 'P', '.S', '.', 'ĠI', 'Ġunderstand', 'Ġthat', 'Ġthe', 'Ġauthenticity', 'Ġof', 'Ġthe', 'Ġpicture', 'Ġis', 'Ġopen', 'Ġto', 'Ġquestion', '.', 'ĠWhoever', 'Ġit', 'Ġactually', 'Ġis', ',', 'Ġhe', 'Ġlooks', 'Âł', 'Ġto', 'Ġme', 'Ġa', 'Ġbit', 'Ġlike', 'ĠLaure', 'nce', 'ĠOlivier', 'âĢ¦ĊĊ', '11', 'ĠResponses', 'Ġto', 'ĠâĢľ', 'Bay', 'es', 'Ġand', 'Ġhis', 'Âł', 'The', 'orem', 'âĢĿĊĊ', '1', '.', 'ĠBry', 'n', 'ĠJones', 'ĠSays', ':ĊĊ', 'The', 'ĠRoyal', 'ĠSociety', 'Ġis', 'Ġproviding', 'Ġfree', 'Ġaccess', 'Ġto', 'Ġelectronic', 'Ġversions', 'Ġof', 'Ġits', 'Ġjournals', 'Ġuntil', 'Ġthe', 'Ġend', 'Ġof', 'Ġthis', 'Ġmonth', '.', 'ĠReaders', 'Ġof', 'Ġthis', 'Ġblog', 'Ġmight', 'Ġlike', 'Ġto', 'Ġlook', 'Ġat', 'ĠThomas', 'ĠBay', 'es', 'âĢĻs', 'Ġtwo', 'Ġpost', 'hum', 'ous', 'Ġpublications', 'Ġin', 'Ġthe', 'ĠPhilosoph', 'ical', 'ĠTransactions', '.ĊĊ', 'The', 'Ġfirst', 'Ġis', 'Ġa', 'Ġshort', 'Ġpaper', 'Ġabout', 'Ġseries', '.', 'ĠThe', 'Ġother', 'Ġis', 'Ġthe', 'Ġpaper', 'Ġabout', 'Ġstatistics', 'Ġcommunicated', 'Ġby', 'ĠRichard', 'ĠPrice', '.', 'Ġ(', 'The', 'Ġstatistics', 'Ġpaper', 'Ġmay', 'Ġbe', 'Ġaccessible', 'Ġon', 'Ġa', 'Ġlong', '-term', 'Ġbasis', 'Ġbecause', 'Ġit', 'Ġis', 'Ġone', 'Ġof', 'Ġthe', 'ĠRoyal', 'ĠSociety', 'âĢĻs', 'ĠTrail', 'bl', 'azing', 'Ġpapers', 'Ġthe', 'Ġsociety', 'Ġprovides', 'Ġaccess', 'Ġto', 'Ġas', 'Ġpart', 'Ġof', 'Ġits', 'Ġ', '350', 'th', 'Ġanniversary', 'Ġcelebrations', '.)ĊĊ', 'Inc', 'identally', ',', 'Ġboth', 'ĠThomas', 'ĠBay', 'es', 'Ġand', 'ĠRichard', 'ĠPrice', 'Ġwere', 'Ġburied', 'Ġin', 'Ġthe', 'ĠBun', 'hill', 'ĠFields', 'ĠCemetery', 'Ġin', 'ĠLondon', 'Ġand', 'Ġtheir', 'Ġtom', 'bs', 'Ġcan', 'Ġbe', 'Ġseen', 'Ġthere', 'Ġtoday', '.ĊĊ', '2', '.', 'ĠSteve', 'ĠWarren', 'ĠSays', ':ĊĊ', 'You', 'Ġmay', 'Ġbe', 'Ġremembered', 'Ġin', 'Ġhistory', 'Ġas', 'Ġthe', 'Ġdiscover', 'er', 'Ġof', 'Ġco', 'les', 'law', ',', 'Ġbut', 'Ġyou', 'Ġweren', 'âĢĻt', 'Ġthe', 'Ġfirst', '.ĊĊ', 'âĢ¢', 'ĠAnton', 'ĠGarrett', 'ĠSays', ':ĊĊ', 'For', 'Ġyears', 'ĠI', 'Ġthought', 'Ġit', 'Ġwas', 'ĠâĢľ', 'cold', 'Ġsl', 'aw', 'âĢĿ', 'Ġbecause', 'Ġit', 'Ġwas', 'Ġserved', 'Ġcold', '.', 'ĠA', 'Ġgood', 'Ġjob', 'ĠI', 'Ġnever', 'Ġasked', 'Ġfor', 'Ġwarm', 'Ġsl', 'aw', '.ĊĊ', '3', '.', 'Ġtelesc', 'oper', 'ĠSays', ':ĊĊ', 'My', 'Ġsurname', ',', 'Ġin', 'ĠSpanish', ',', 'Ġmeans', 'ĠâĢľ', 'C', 'abb', 'ages', 'âĢĿ.', 'ĠSo', 'Ġit', 'Ġwas', 'Ġprobably', 'Ġone', 'Ġof', 'Ġmy', 'Ġancestors', 'Ġwho', 'Ġinvented', 'Ġthe', 'Ġchopped', 'Ġvariety', '.ĊĊ', '4', '.', 'ĠAnton', 'ĠGarrett', 'ĠSays', ':ĊĊ', 'Thomas', 'ĠBay', 'es', 'Ġis', 'Ġnow', 'Ġknown', 'Ġto', 'Ġhave', 'Ġgone', 'Ġto', 'ĠEdinburgh', 'ĠUniversity', ',', 'Ġwhere', 'Ġhis', 'Ġname', 'Ġappears', 'Ġin', 'Ġthe', 'Ġrecords', '.', 'ĠHe', 'Ġwas', 'Ġbarred', 'Ġfrom', 'ĠEnglish', 'Ġuniversities', 'Ġbecause', 'Ġhis', 'Ġnon', 'con', 'form', 'ist', 'Ġfamily', 'Ġdid', 'Ġnot', 'Ġhave', 'Ġhim', 'Ġbapt', 'ised', 'Ġin', 'Ġthe', 'ĠChurch', 'Ġof', 'ĠEngland', '.', 'Ġ(', 'Charles', 'ĠDarwin', 'âĢĻs', 'Ġnon', 'con', 'form', 'ist', 'Ġfamily', 'Ġcovered', 'Ġtheir', 'Ġbets', 'Ġby', 'Ġhaving', 'Ġbaby', 'ĠCharles', 'Ġbapt', 'ised', 'Ġin', 'Ġthe', 'ĠCo', 'E', ',', 'Ġalthough', 'Ġperhaps', 'Ġthey', 'Ġbelieved', 'Ġit', 'Ġdidn', 'âĢĻt', 'Ġcount', 'Ġas', 'Ġa', 'Ġbaptism', 'Ġsince', 'ĠCharles', 'Ġhad', 'Ġno', 'Ġsay', 'Ġin', 'Ġit', '.', 'ĠT', 'ist', 'Ġis', 'Ġwhy', 'Ġhe', 'Ġwas', 'Ġable', 'Ġto', 'Ġgo', 'Ġto', 'ĠChrist', 'âĢĻs', 'ĠCollege', ',', 'ĠCambridge', '.)ĊĊ', '5', '.', 'ĠâĢľ', 'Cole', 'âĢĿ', 'Ġis', 'Ġan', 'Ġold', 'ĠEnglish', 'Ġword', 'Ġfor', 'Ġcabbage', ',', 'Ġwhich', 'Ġsurvives', 'Ġin', 'ĠâĢľ', 'cole', 'Ġsl', 'aw', 'âĢĿ.', 'ĠThe', 'ĠGerman', 'Ġword', 'Ġis', 'ĠâĢľ', 'K', 'ohl', 'âĢĿ.', 'Ġ(', 'Some', 'how', ',', 'ĠI', 'Ġdon', 'âĢĻt', 'Ġsee', 'ĠPM', 'Ġor', 'ĠPresident', 'ĠC', 'abbage', 'Ġbeing', 'Ġa', 'Ġrealistic', 'Ġpossibility', '.', 'ĠðŁĻĤ', 'Ġ)ĊĊ', 'Note', 'Ġthat', 'ĠOld', 'ĠKing', 'ĠCole', 'Ġis', 'Ġunrelated', 'Ġ(', 'et', 'ym', 'ologically', ').', 'ĠOf', 'Ġcourse', ',', 'Ġthis', 'Ġdiscussion', 'Ġcould', 'Ġcause', 'ĠPeter', 'Ġto', 'Ġpost', 'Ġa', 'Ġclip', 'Ġof', 'ĊĊ', 'Nat', 'ĠâĢľ', 'King', 'âĢĿ', 'ĠCole', 'Ċ', '(guess', 'Ġwhat', 'Ġhis', 'Ġreal', 'Ġsurname', 'Ġis', ').ĊĊ', 'To', 'Ġremind', 'Ġpeople', 'Ġto', 'Ġpay', 'Ġattention', 'Ġto', 'Ġspelling', 'Ġwhen', 'Ġthey', 'Ġhear', 'Ġwords', ',', 'Ġwe', 'âĢĻll', 'Ġclose', 'Ġwith', 'Ġthe', 'ĠQuote', 'Ġof', 'Ġthe', 'ĠDay', ':ĊĊ', 'It', 'âĢĻs', 'Ġimportant', 'Ġto', 'Ġpay', 'Ġclose', 'Ġattention', 'Ġin', 'Ġschool', '.', 'ĠFor', 'Ġyears', 'ĠI', 'Ġthought', 'Ġthat', 'Ċ', 'b', 'ears', 'Ġmasturb', 'ated', 'Ġall', 'Ġwinter', '.ĊĊ', 'âĢĶ', 'D', 'amon', 'ĠR', '.', 'ĠMil', 'hem', 'ĊĊ', '6', '.', 'ĠOf', 'Ġcourse', ',', 'Ġthis', 'Ġdiscussion', 'Ġcould', 'Ġcause', 'ĠPeter', 'Ġto', 'Ġpost', 'Ġa', 'Ġclip', 'Ġof', 'Ċ', 'Nat', 'ĠKing', 'ĠCole', 'Ċ', '(g', 'i', 'ess', 'Ġwhat', 'Ġhis', 'Ġreal', 'Ġsurname', 'Ġis', ').ĊĊ', '7', '.', 'ĠOf', 'Ġcourse', ',', 'Ġthis', 'Ġdiscussion', 'Ġcould', 'Ġcause', 'ĠPeter', 'Ġto', 'Ġpost', 'Ġa', 'Ġclip', 'Ġof', 'Ċ', 'Nat', 'ĠKing', 'ĠCole', 'Ċ', '(g', 'i', 'ess', 'Ġwhat', 'Ġhis', 'Ġreal', 'Ġsurname', 'Ġis', ').ĊĊ', 'The', 'Ġfirst', 'Ġtypo', 'Ġwas', 'Ġmy', 'Ġfault', ';', 'Ġthe', 'Ġextra', 'Ġline', 'break', 's', 'Ġin', 'Ġthe', 'Ġsecond', 'Ġattempt', 'Ċ', '(test', 'ed', 'Ġagain', 'Ġhere', ')', 'Ġappear', 'Ġto', 'Ġbe', 'Ġa', 'Ġnew', 'ĠâĢľ', 'feature', 'âĢĿ.ĊĊ', '8', '.', 'Ġtelesc', 'oper', 'ĠSays', ':ĊĊ', 'The', 'Ġnoun', 'ĠâĢľ', 'cole', 'âĢĿ', 'Ġcan', 'Ġbe', 'Ġfound', 'Ġin', 'ĠEnglish', 'Ġdictionaries', 'Ġas', 'Ġa', 'Ġgeneric', 'Ġname', 'Ġfor', 'Ġplants', 'Ġof', 'Ġthe', 'Ġcabbage', 'Ġfamily', '.', 'ĠIt', 'Ġis', 'Ġrelated', 'Ġto', 'Ġthe', 'ĠGerman', 'Ġk', 'ohl', 'Ġand', 'Ġsc', 'ott', 'ish', 'Ġk', 'ail', 'Ġor', 'Ġkale', '.', 'ĠThese', 'Ġare', 'Ġall', 'Ġderived', 'Ġfrom', 'Ġthe', 'Ġlatin', 'Ġword', 'Ġcol', 'is', 'Ġ(', 'or', 'Ġca', 'ulis', ')', 'Ġmeaning', 'Ġa', 'Ġstem', ',', 'Ġwhich', 'Ġis', 'Ġalso', 'Ġthe', 'Ġroot', 'Ġof', 'Ġthe', 'Ġword', 'Ġcauliflower', '.ĊĊ', 'The', 'Ġsurname', 'ĠâĢľ', 'Cole', 'âĢĿ', 'Ġand', 'Ġthe', 'Ġvariant', 'ĠâĢľ', 'C', 'oles', 'âĢĿ', 'Ġare', 'Ġfairly', 'Ġcommon', 'Ġin', 'ĠEngland', 'Ġand', 'ĠWales', ',', 'Ġbut', 'Ġare', 'Ġnot', 'Ġrelated', 'Ġto', 'Ġthe', 'Ġlatin', 'Ġword', 'Ġfor', 'Ġcabbage', '.', 'ĠBoth', 'Ġare', 'Ġdimin', 'ut', 'ives', 'Ġof', 'Ġthe', 'Ġname', 'ĠâĢľ', 'Nich', 'olas', 'âĢĿ.ĊĊ', '9', '.', 'Ġ[âĢ¦]', 'ĠI', 'Ġposted', 'Ġa', 'Ġlittle', 'Ġpiece', 'Ġabout', 'ĠBayesian', 'Ġprobability', '.', 'ĠThat', 'Ġone', 'Ġand', 'Ġthe', 'Ġothers', 'Ġthat', 'Ġfollowed', 'Ġit', 'Ġ(', 'here', 'Ġand', 'Ġhere', ')', 'Ġproved', 'Ġto', 'Ġbe', 'Ġsurprisingly', 'Ġpopular', 'Ġso', 'ĠI', 'âĢĻve', 'Ġbeen', 'Ġplanning', 'Ġto', 'Ġadd', 'Ġa', 'Ġfew', 'Ġmore', 'Ġposts', 'Ġ[âĢ¦]ĊĊ', '10', '.', 'ĠIt', 'Ġalready', 'Ġhas', 'Ġa', 'Ġpopular', 'Ġname', ':', 'ĠSt', 'ig', 'ler', 'âĢĻs', 'Ġlaw', 'Ġof', 'Ġe', 'pon', 'ym', 'y', '.']\n",
      "Token IDs:\n",
      "[23407, 288, 323, 813, 4194, 791, 13475, 271, 5159, 6931, 1772, 389, 99234, 19463, 5084, 311, 617, 8066, 5115, 264, 2763, 315, 13016, 11, 779, 420, 16163, 1712, 358, 3463, 358, 7070, 923, 264, 2697, 2766, 315, 4092, 13, 578, 3766, 10430, 3940, 505, 279, 1121, 271, 3, 47, 5462, 91, 1741, 8, 284, 735, 88310, 16, 92, 47, 5462, 91, 34, 8, 47, 4444, 91, 5002, 8, 284, 735, 88310, 16, 92, 393, 7, 1905, 91, 34, 15437, 271, 2940, 271, 3, 42, 77878, 4444, 91, 34, 570, 67526, 16179, 420, 374, 2663, 9332, 288, 529, 58917, 11, 279, 4689, 1376, 315, 433, 439, 11224, 1618, 574, 3604, 1176, 5439, 1523, 11, 539, 555, 9332, 288, 719, 555, 50155, 27634, 13, 3639, 9332, 288, 529, 1550, 574, 43530, 279, 3361, 1162, 315, 420, 15150, 369, 1054, 258, 50517, 863, 279, 9736, 21524, 8141, 13, 1115, 8141, 6835, 279, 19463, 315, 865, 48188, 304, 308, 9678, 1054, 376, 10522, 863, 1855, 3515, 279, 1890, 19463, 315, 2450, 11, 281, 26, 1855, 1054, 48447, 863, 706, 1193, 1403, 3284, 20124, 27179, 5748, 863, 477, 1054, 30039, 65312, 70544, 1093, 420, 527, 6118, 2663, 14502, 283, 41076, 19622, 11, 1306, 15469, 14502, 283, 41076, 13, 1442, 584, 2610, 279, 3488, 1054, 12840, 374, 279, 19463, 315, 7041, 865, 48188, 505, 279, 3284, 308, 30, 9520, 279, 4320, 374, 2728, 555, 279, 9736, 21524, 8141, 1473, 3, 47, 1107, 2120, 91, 77, 7385, 11992, 356, 1471, 13095, 8, 281, 61, 87, 320, 16, 2320, 30876, 90, 77, 6695, 32816, 271, 2940, 271, 3, 34, 1471, 13095, 11992, 308, 89080, 87, 10509, 77, 6695, 42395, 67526, 285, 279, 1396, 315, 12742, 28559, 315, 865, 6302, 430, 649, 387, 15107, 505, 264, 7463, 315, 308, 382, 2675, 649, 4762, 1518, 7214, 1268, 420, 48282, 13, 578, 19463, 315, 865, 24871, 48188, 374, 281, 56016, 555, 5196, 865, 3115, 11, 477, 17585, 13, 578, 19463, 315, 320, 77, 6695, 8, 50024, 28950, 374, 30293, 320, 16, 2320, 80198, 6695, 13, 578, 1566, 1403, 3878, 13524, 9093, 3371, 603, 279, 19463, 430, 584, 617, 7041, 865, 48188, 320, 11536, 1070, 2011, 387, 308, 6695, 28950, 570, 578, 3698, 17720, 532, 8331, 304, 4156, 5097, 2759, 315, 279, 2144, 430, 279, 22106, 315, 48188, 323, 28950, 3250, 1431, 5030, 382, 791, 9736, 21524, 8141, 17208, 11, 369, 3187, 11, 311, 11763, 26251, 288, 315, 264, 16652, 11, 304, 902, 1162, 281, 374, 4529, 311, 387, 220, 15, 13, 20, 369, 264, 6762, 16652, 13, 362, 48761, 16652, 2643, 617, 264, 2204, 907, 315, 281, 11, 719, 439, 1317, 439, 279, 26251, 288, 527, 9678, 279, 15150, 2103, 17208, 13, 578, 9736, 21524, 8141, 1101, 17208, 311, 5435, 16239, 13633, 20953, 505, 66967, 82, 25, 433, 4375, 7041, 422, 279, 20953, 527, 12860, 304, 279, 66967, 1306, 1855, 4128, 11, 719, 433, 1101, 17208, 13489, 2085, 14039, 11, 439, 1317, 439, 279, 1396, 315, 27741, 374, 1790, 9333, 1109, 279, 1396, 315, 20953, 304, 279, 66967, 13, 358, 5387, 433, 439, 459, 10368, 311, 11294, 279, 31293, 907, 315, 279, 9736, 21524, 8141, 11, 719, 279, 1121, 374, 539, 15206, 25, 469, 7799, 11992, 6331, 13, 1442, 499, 26251, 264, 6762, 16652, 5899, 3115, 279, 31293, 907, 369, 279, 1396, 315, 14971, 374, 220, 605, 3115, 220, 15, 13, 20, 11, 902, 374, 4330, 13, 2360, 13051, 1070, 13, 4740, 2500, 2766, 315, 71808, 11, 279, 33373, 315, 279, 8141, 649, 1101, 387, 1766, 13, 1102, 374, 2660, 7, 16, 2320, 3677, 4516, 420, 6835, 603, 279, 19463, 315, 865, 2728, 264, 8521, 907, 315, 281, 13, 9332, 288, 574, 8173, 304, 279, 29049, 315, 420, 1121, 11, 279, 19463, 315, 281, 2728, 865, 13, 763, 1023, 4339, 11, 9332, 288, 574, 8173, 304, 279, 4320, 311, 279, 3488, 1054, 2746, 358, 2804, 308, 9678, 19622, 323, 636, 865, 48188, 11, 1148, 374, 279, 19463, 8141, 315, 281, 30, 11453, 1115, 374, 264, 11670, 3187, 315, 29049, 33811, 13, 1283, 2751, 279, 4495, 4320, 11, 9778, 11, 719, 555, 1633, 5804, 337, 2844, 33811, 13, 763, 856, 9647, 433, 374, 5115, 5107, 311, 9541, 279, 836, 9332, 288, 529, 58917, 3196, 389, 1148, 568, 3604, 1550, 11, 8051, 50155, 27634, 1550, 11951, 25670, 420, 19035, 994, 568, 14592, 279, 4689, 1121, 3010, 11, 902, 374, 912, 10712, 3249, 279, 58917, 374, 2744, 7086, 304, 9332, 288, 529, 34662, 382, 2028, 374, 539, 279, 1193, 3187, 304, 8198, 1405, 279, 5076, 1732, 753, 836, 374, 12673, 311, 264, 1121, 477, 18841, 13, 763, 2144, 11, 433, 374, 4661, 264, 2383, 315, 22037, 430, 904, 58917, 430, 706, 264, 836, 706, 279, 5076, 836, 13, 358, 30714, 430, 420, 22695, 1288, 16472, 71627, 387, 3967, 439, 3623, 645, 529, 7658, 382, 4516, 889, 574, 279, 26454, 21651, 1122, 4920, 420, 1121, 30, 11355, 9332, 288, 574, 9405, 304, 220, 8258, 17, 11, 4538, 315, 40592, 9332, 288, 11, 889, 574, 264, 37946, 315, 279, 16591, 13581, 320, 10725, 50, 8, 323, 832, 315, 279, 1633, 1176, 2536, 444, 630, 380, 35050, 311, 387, 86724, 304, 9635, 13, 11355, 574, 5678, 86724, 323, 369, 264, 1418, 6575, 449, 813, 7126, 304, 279, 86464, 30155, 4783, 304, 32178, 27109, 11, 3221, 16071, 16381, 304, 7295, 13, 763, 220, 10861, 15, 568, 574, 264, 13015, 304, 28556, 14024, 37958, 11, 304, 18206, 13, 1283, 22311, 505, 279, 8993, 304, 220, 10005, 17, 323, 8636, 304, 220, 10967, 16, 13, 11355, 9332, 288, 3287, 1431, 3498, 264, 3254, 5684, 389, 38696, 304, 813, 1866, 836, 2391, 813, 19569, 719, 8994, 420, 574, 16689, 264, 37946, 315, 279, 16591, 13581, 320, 10725, 50, 8, 304, 220, 11771, 17, 13, 4203, 76051, 568, 1047, 23323, 315, 279, 10291, 16347, 13, 1283, 1550, 4869, 3350, 264, 5684, 389, 31405, 919, 304, 220, 11908, 21, 11, 902, 574, 4756, 73251, 13, 1115, 574, 4762, 279, 21319, 389, 902, 568, 574, 16689, 459, 435, 11706, 382, 791, 5684, 8649, 279, 58917, 430, 1457, 30824, 813, 836, 574, 4756, 1772, 28400, 7162, 304, 279, 38356, 950, 56385, 315, 279, 16591, 13581, 315, 7295, 304, 220, 10967, 19, 382, 47, 815, 13, 358, 3619, 430, 279, 54348, 315, 279, 6945, 374, 1825, 311, 3488, 13, 91395, 433, 3604, 374, 11, 568, 5992, 4194, 311, 757, 264, 2766, 1093, 33767, 16848, 78118, 5551, 806, 81567, 311, 1054, 23407, 288, 323, 813, 4194, 791, 13475, 7663, 16, 13, 19803, 77, 12201, 47559, 1473, 791, 16591, 13581, 374, 8405, 1949, 2680, 311, 14683, 11028, 315, 1202, 42780, 3156, 279, 842, 315, 420, 2305, 13, 65514, 315, 420, 5117, 2643, 1093, 311, 1427, 520, 11355, 9332, 288, 753, 1403, 1772, 28400, 788, 29085, 304, 279, 38356, 950, 56385, 382, 791, 1176, 374, 264, 2875, 5684, 922, 4101, 13, 578, 1023, 374, 279, 5684, 922, 13443, 59035, 555, 12131, 8650, 13, 320, 791, 13443, 5684, 1253, 387, 15987, 389, 264, 1317, 9860, 8197, 1606, 433, 374, 832, 315, 279, 16591, 13581, 753, 19177, 2067, 6795, 16064, 279, 8396, 5825, 2680, 311, 439, 961, 315, 1202, 220, 8652, 339, 22310, 47674, 9456, 40345, 57222, 11, 2225, 11355, 9332, 288, 323, 12131, 8650, 1051, 28016, 304, 279, 66347, 28607, 25599, 59578, 304, 7295, 323, 872, 10390, 1302, 649, 387, 3970, 1070, 3432, 382, 17, 13, 14129, 26713, 47559, 1473, 2675, 1253, 387, 27569, 304, 3925, 439, 279, 7142, 261, 315, 1080, 645, 20510, 11, 719, 499, 15058, 1431, 279, 1176, 382, 6806, 17958, 58716, 47559, 1473, 2520, 1667, 358, 3463, 433, 574, 1054, 88172, 1776, 675, 863, 1606, 433, 574, 10434, 9439, 13, 362, 1695, 2683, 358, 2646, 4691, 369, 8369, 1776, 675, 382, 18, 13, 78513, 3376, 47559, 1473, 5159, 40853, 11, 304, 15506, 11, 3445, 1054, 34, 12806, 1154, 11453, 2100, 433, 574, 4762, 832, 315, 856, 38618, 889, 36592, 279, 38525, 8205, 382, 19, 13, 17958, 58716, 47559, 1473, 42493, 9332, 288, 374, 1457, 3967, 311, 617, 8208, 311, 41406, 3907, 11, 1405, 813, 836, 8111, 304, 279, 7576, 13, 1283, 574, 60044, 505, 6498, 23978, 1606, 813, 2536, 444, 630, 380, 3070, 1550, 539, 617, 1461, 43724, 4147, 304, 279, 9441, 315, 9635, 13, 320, 54567, 45480, 753, 2536, 444, 630, 380, 3070, 9960, 872, 43177, 555, 3515, 8945, 15274, 43724, 4147, 304, 279, 3623, 36, 11, 8051, 8530, 814, 11846, 433, 3287, 1431, 1797, 439, 264, 74927, 2533, 15274, 1047, 912, 2019, 304, 433, 13, 350, 380, 374, 3249, 568, 574, 3025, 311, 733, 311, 3771, 753, 9304, 11, 24562, 9456, 20, 13, 1054, 99280, 863, 374, 459, 2362, 6498, 3492, 369, 74873, 11, 902, 83417, 304, 1054, 56745, 1776, 675, 11453, 578, 6063, 3492, 374, 1054, 42, 40437, 11453, 320, 8538, 5269, 11, 358, 1541, 1431, 1518, 5975, 477, 4900, 356, 57589, 1694, 264, 26569, 13336, 13, 28584, 5235, 9290, 430, 10846, 6342, 24298, 374, 46305, 320, 295, 1631, 30450, 570, 5046, 3388, 11, 420, 10430, 1436, 5353, 11291, 311, 1772, 264, 12607, 315, 271, 66314, 1054, 34655, 863, 24298, 198, 99402, 1148, 813, 1972, 40853, 374, 3677, 1271, 24928, 1274, 311, 2343, 6666, 311, 43529, 994, 814, 6865, 4339, 11, 584, 4805, 3345, 449, 279, 25552, 315, 279, 6187, 1473, 2181, 753, 3062, 311, 2343, 3345, 6666, 304, 2978, 13, 1789, 1667, 358, 3463, 430, 198, 65, 7596, 30443, 660, 682, 12688, 382, 2345, 35, 31936, 432, 13, 10357, 30132, 271, 21, 13, 5046, 3388, 11, 420, 10430, 1436, 5353, 11291, 311, 1772, 264, 12607, 315, 198, 66314, 6342, 24298, 198, 3348, 72, 434, 1148, 813, 1972, 40853, 374, 3677, 22, 13, 5046, 3388, 11, 420, 10430, 1436, 5353, 11291, 311, 1772, 264, 12607, 315, 198, 66314, 6342, 24298, 198, 3348, 72, 434, 1148, 813, 1972, 40853, 374, 3677, 791, 1176, 86205, 574, 856, 14867, 26, 279, 5066, 1584, 9137, 82, 304, 279, 2132, 4879, 198, 8793, 291, 1578, 1618, 8, 5101, 311, 387, 264, 502, 1054, 13043, 15397, 23, 13, 78513, 3376, 47559, 1473, 791, 38021, 1054, 56745, 863, 649, 387, 1766, 304, 6498, 58614, 439, 264, 14281, 836, 369, 11012, 315, 279, 74873, 3070, 13, 1102, 374, 5552, 311, 279, 6063, 597, 40437, 323, 1156, 1751, 819, 597, 607, 477, 63577, 13, 4314, 527, 682, 14592, 505, 279, 64019, 3492, 1400, 285, 320, 269, 2211, 65130, 8, 7438, 264, 19646, 11, 902, 374, 1101, 279, 3789, 315, 279, 3492, 96970, 382, 791, 40853, 1054, 99280, 863, 323, 279, 11678, 1054, 34, 7298, 863, 527, 14470, 4279, 304, 9635, 323, 23782, 11, 719, 527, 539, 5552, 311, 279, 64019, 3492, 369, 74873, 13, 11995, 527, 48416, 332, 1924, 315, 279, 836, 1054, 86279, 19736, 15397, 24, 13, 28624, 358, 8621, 264, 2697, 6710, 922, 99234, 19463, 13, 3011, 832, 323, 279, 3885, 430, 8272, 433, 320, 6881, 323, 1618, 8, 19168, 311, 387, 29392, 5526, 779, 358, 4070, 1027, 9293, 311, 923, 264, 2478, 810, 8158, 12996, 605, 13, 1102, 2736, 706, 264, 5526, 836, 25, 800, 343, 1565, 753, 2383, 315, 384, 621, 1631, 88, 13]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Tokenize a sample text\n",
    "sample_text = dataset[\"train\"][0][\"text\"]\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"Token IDs:\")\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56151fab48f045238548cc806281e03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d95172db6b40a1878a7d8e8aed3184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on small sample...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 16.91 GB, other allocations: 1.05 GB, max allowed: 18.13 GB). Tried to allocate 1002.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training on small sample...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/transformers/trainer.py:2171\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2172\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/transformers/trainer.py:2531\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2524\u001b[39m context = (\n\u001b[32m   2525\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2526\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2527\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2529\u001b[39m )\n\u001b[32m   2530\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2531\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2534\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2535\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2536\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2537\u001b[39m ):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2539\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/transformers/trainer.py:3712\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3710\u001b[39m     loss = loss / \u001b[38;5;28mself\u001b[39m.args.gradient_accumulation_steps\n\u001b[32m-> \u001b[39m\u001b[32m3712\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3714\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/accelerate/accelerator.py:2248\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2246\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2247\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2248\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 16.91 GB, other allocations: 1.05 GB, max allowed: 18.13 GB). Tried to allocate 1002.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "split_dataset = dataset[\"train\"].train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "small_train_dataset = train_dataset.select(range(100))\n",
    "small_eval_dataset = eval_dataset.select(range(50))\n",
    "\n",
    "# Assign a padding token to the tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use the eos_token as the pad_token\n",
    "\n",
    "# Tokenize the small sample dataset\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()  # Add labels for loss computation\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-math-pretrain-small\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10, \n",
    "    save_steps=10, \n",
    "    per_device_train_batch_size=1, \n",
    "    num_train_epochs=1,  # Train for 1 epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if supported\n",
    "    report_to=\"none\",  # Disable reporting to external services\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training on small sample...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Attention Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(32, 15, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Visualize attention for the first layer and first head\u001b[39;00m\n\u001b[32m     14\u001b[39m attention = attention_scores[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m].detach().numpy()  \u001b[38;5;66;03m# First layer, first head\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43msns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mviridis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mAttention Scores\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m plt.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/seaborn/matrix.py:446\u001b[39m, in \u001b[36mheatmap\u001b[39m\u001b[34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[32m    366\u001b[39m \n\u001b[32m    367\u001b[39m \u001b[33;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m \n\u001b[32m    444\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m plotter = \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[32m    451\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mlinewidths\u001b[39m\u001b[33m\"\u001b[39m] = linewidths\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/seaborn/matrix.py:110\u001b[39m, in \u001b[36m_HeatMapper.__init__\u001b[39m\u001b[34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    109\u001b[39m     plot_data = np.asarray(data)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     data = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[32m    113\u001b[39m mask = _matrix_mask(data, mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/pandas/core/frame.py:827\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    816\u001b[39m         mgr = dict_to_mgr(\n\u001b[32m    817\u001b[39m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[32m    818\u001b[39m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    824\u001b[39m             copy=_copy,\n\u001b[32m    825\u001b[39m         )\n\u001b[32m    826\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m827\u001b[39m         mgr = \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:314\u001b[39m, in \u001b[36mndarray_to_mgr\u001b[39m\u001b[34m(values, index, columns, dtype, copy, typ)\u001b[39m\n\u001b[32m    308\u001b[39m     _copy = (\n\u001b[32m    309\u001b[39m         copy_on_sanitize\n\u001b[32m    310\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m astype_is_view(values.dtype, dtype))\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    312\u001b[39m     )\n\u001b[32m    313\u001b[39m     values = np.array(values, copy=_copy)\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     values = \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[32m    319\u001b[39m     values = _prep_ndarraylike(values, copy=copy_on_sanitize)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniforge/base/envs/nlp-env/lib/python3.11/site-packages/pandas/core/internals/construction.py:592\u001b[39m, in \u001b[36m_ensure_2d\u001b[39m\u001b[34m(values)\u001b[39m\n\u001b[32m    590\u001b[39m     values = values.reshape((values.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m))\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m values.ndim != \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[31mValueError\u001b[39m: Must pass 2-d input. shape=(32, 15, 15)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Example input\n",
    "inputs = tokenizer(\"Solve for x: 2x + 3 = 7\", return_tensors=\"pt\")\n",
    "\n",
    "# Forward pass with attention outputs\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Access attention scores\n",
    "attention_scores = outputs.attentions  # List of attention scores for each layer\n",
    "\n",
    "# Visualize attention for the first layer and first head\n",
    "attention = attention_scores[0][0].detach().numpy()  # First layer, first head\n",
    "sns.heatmap(attention, cmap=\"viridis\")\n",
    "plt.title(\"Attention Scores\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
